{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sKCAHmIr3nQ1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 1: Building the Core Architecture\n",
        "\n",
        "This notebook covers the main part of the project: implementing the custom Swin Transformer model. The first step is to define all the custom layers needed, like PatchEmbedding, WindowAttention, and the main SwinTransformer block itself. This is the foundation of the model."
      ],
      "metadata": {
        "id": "Hy7m2rRX3w6g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras import layers\n",
        "from keras import ops\n",
        "import os\n",
        "\n",
        "# --- Model Hyperparameters ---\n",
        "num_classes = 5\n",
        "input_shape = (224, 224, 3)\n",
        "patch_size = (4, 4)\n",
        "num_heads = 6\n",
        "embed_dim = 96\n",
        "num_mlp = 384\n",
        "qkv_bias = True\n",
        "window_size = 7\n",
        "shift_size = 3\n",
        "image_dimension = 224\n",
        "\n",
        "# --- Calculate patch details ---\n",
        "num_patch_x = input_shape[0] // patch_size[0]\n",
        "num_patch_y = input_shape[1] // patch_size[1]\n",
        "print(f\"Number of patches: {num_patch_x}x{num_patch_y} = {num_patch_x * num_patch_y}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lz0faHNZ3xUR",
        "outputId": "56186dc7-e72d-48ef-d5cf-9350cd4c2176"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of patches: 56x56 = 3136\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Helper Functions\n",
        "\n",
        "These two functions are used to partition the image into windows and then reverse the proccess. This is a core idea of the Swin Transformer to compute attention locally."
      ],
      "metadata": {
        "id": "XCG0d0m-35-X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def window_partition(x, window_size):\n",
        "    _, height, width, channels = x.shape\n",
        "    patch_num_y = height // window_size\n",
        "    patch_num_x = width // window_size\n",
        "    x = ops.reshape(\n",
        "        x, (-1, patch_num_y, window_size, patch_num_x, window_size, channels)\n",
        "    )\n",
        "    x = ops.transpose(x, (0, 1, 3, 2, 4, 5))\n",
        "    windows = ops.reshape(x, (-1, window_size, window_size, channels))\n",
        "    return windows\n",
        "\n",
        "def window_reverse(windows, window_size, height, width, channels):\n",
        "    patch_num_y = height // window_size\n",
        "    patch_num_x = width // window_size\n",
        "    x = ops.reshape(\n",
        "        windows, (-1, patch_num_y, patch_num_x, window_size, window_size, channels)\n",
        "    )\n",
        "    x = ops.transpose(x, (0, 1, 3, 2, 4, 5))\n",
        "    x = ops.reshape(x, (-1, height, width, channels))\n",
        "    return x"
      ],
      "metadata": {
        "id": "VfmdZXOy36Vm"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Core Layers Implementation\n",
        "\n",
        "Now, I'm defining the main custom layers for the model.\n",
        "\n",
        "1.  **WindowAttention**: Computes self-attention within local windows.\n",
        "2.  **SwinTransformer**: The main block that combines window attention with the MLP.\n",
        "3.  **PatchEmbedding**: Converts image patches to vector embeddings.\n",
        "4.  **PatchMerging**: Downsamples the image by merging patches, which creates the hirearchical structure."
      ],
      "metadata": {
        "id": "FoXAtMZ33_DP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class WindowAttention(layers.Layer):\n",
        "    def __init__(self, dim, window_size, num_heads, qkv_bias=True, dropout_rate=0.0, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.dim = dim\n",
        "        self.window_size = window_size\n",
        "        self.num_heads = num_heads\n",
        "        self.scale = (dim // num_heads) ** -0.5\n",
        "        self.qkv = layers.Dense(dim * 3, use_bias=qkv_bias)\n",
        "        self.dropout = layers.Dropout(dropout_rate)\n",
        "        self.proj = layers.Dense(dim)\n",
        "\n",
        "        # Define relative position bias\n",
        "        num_window_elements = (2 * self.window_size[0] - 1) * (2 * self.window_size[1] - 1)\n",
        "        self.relative_position_bias_table = self.add_weight(\n",
        "            shape=(num_window_elements, self.num_heads),\n",
        "            initializer=keras.initializers.Zeros(),\n",
        "            trainable=True,\n",
        "        )\n",
        "        coords_h = np.arange(self.window_size[0])\n",
        "        coords_w = np.arange(self.window_size[1])\n",
        "        coords_matrix = np.meshgrid(coords_h, coords_w, indexing=\"ij\")\n",
        "        coords = np.stack(coords_matrix)\n",
        "        coords_flatten = coords.reshape(2, -1)\n",
        "        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]\n",
        "        relative_coords = relative_coords.transpose([1, 2, 0])\n",
        "        relative_coords[:, :, 0] += self.window_size[0] - 1\n",
        "        relative_coords[:, :, 1] += self.window_size[1] - 1\n",
        "        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n",
        "        relative_position_index = relative_coords.sum(-1)\n",
        "        self.relative_position_index = keras.Variable(\n",
        "            initializer=relative_position_index,\n",
        "            shape=relative_position_index.shape,\n",
        "            dtype=\"int\",\n",
        "            trainable=False,\n",
        "        )\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        _, size, channels = x.shape\n",
        "        head_dim = channels // self.num_heads\n",
        "        x_qkv = self.qkv(x)\n",
        "        x_qkv = ops.reshape(x_qkv, (-1, size, 3, self.num_heads, head_dim))\n",
        "        x_qkv = ops.transpose(x_qkv, (2, 0, 3, 1, 4))\n",
        "        q, k, v = x_qkv[0], x_qkv[1], x_qkv[2]\n",
        "        q = q * self.scale\n",
        "        k = ops.transpose(k, (0, 1, 3, 2))\n",
        "        attn = q @ k\n",
        "\n",
        "        num_window_elements = self.window_size[0] * self.window_size[1]\n",
        "        relative_position_index_flat = ops.reshape(self.relative_position_index, (-1,))\n",
        "        relative_position_bias = ops.take(self.relative_position_bias_table, relative_position_index_flat, axis=0)\n",
        "        relative_position_bias = ops.reshape(relative_position_bias, (num_window_elements, num_window_elements, -1))\n",
        "        relative_position_bias = ops.transpose(relative_position_bias, (2, 0, 1))\n",
        "        attn = attn + ops.expand_dims(relative_position_bias, axis=0)\n",
        "\n",
        "        if mask is not None:\n",
        "            nW = mask.shape[0]\n",
        "            mask_float = ops.cast(ops.expand_dims(ops.expand_dims(mask, axis=1), axis=0), \"float32\")\n",
        "            attn = ops.reshape(attn, (-1, nW, self.num_heads, size, size)) + mask_float\n",
        "            attn = ops.reshape(attn, (-1, self.num_heads, size, size))\n",
        "            attn = keras.activations.softmax(attn, axis=-1)\n",
        "        else:\n",
        "            attn = keras.activations.softmax(attn, axis=-1)\n",
        "\n",
        "        attn = self.dropout(attn)\n",
        "        x_qkv = attn @ v\n",
        "        x_qkv = ops.transpose(x_qkv, (0, 2, 1, 3))\n",
        "        x_qkv = ops.reshape(x_qkv, (-1, size, channels))\n",
        "        x_qkv = self.proj(x_qkv)\n",
        "        x_qkv = self.dropout(x_qkv)\n",
        "        return x_qkv\n",
        "\n",
        "class SwinTransformer(layers.Layer):\n",
        "    def __init__(self, dim, num_patch, num_heads, window_size=7, shift_size=0, num_mlp=1024, qkv_bias=True, dropout_rate=0.0, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.dim = dim\n",
        "        self.num_patch = num_patch\n",
        "        self.num_heads = num_heads\n",
        "        self.window_size = window_size\n",
        "        self.shift_size = shift_size\n",
        "        self.num_mlp = num_mlp\n",
        "        self.norm1 = layers.LayerNormalization(epsilon=1e-5)\n",
        "        self.attn = WindowAttention(\n",
        "            dim,\n",
        "            window_size=(self.window_size, self.window_size),\n",
        "            num_heads=num_heads,\n",
        "            qkv_bias=qkv_bias,\n",
        "            dropout_rate=dropout_rate,\n",
        "        )\n",
        "        self.drop_path = layers.Dropout(dropout_rate)\n",
        "        self.norm2 = layers.LayerNormalization(epsilon=1e-5)\n",
        "        self.mlp = keras.Sequential([\n",
        "            layers.Dense(num_mlp),\n",
        "            layers.Activation(keras.activations.gelu),\n",
        "            layers.Dropout(dropout_rate),\n",
        "            layers.Dense(dim),\n",
        "            layers.Dropout(dropout_rate),\n",
        "        ])\n",
        "        if min(self.num_patch) < self.window_size:\n",
        "            self.shift_size = 0\n",
        "            self.window_size = min(self.num_patch)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        if self.shift_size > 0:\n",
        "            height, width = self.num_patch\n",
        "            h_slices = (slice(0, -self.window_size), slice(-self.window_size, -self.shift_size), slice(-self.shift_size, None))\n",
        "            w_slices = (slice(0, -self.window_size), slice(-self.window_size, -self.shift_size), slice(-self.shift_size, None))\n",
        "            mask_array = np.zeros((1, height, width, 1))\n",
        "            count = 0\n",
        "            for h in h_slices:\n",
        "                for w in w_slices:\n",
        "                    mask_array[:, h, w, :] = count\n",
        "                    count += 1\n",
        "            mask_array = ops.convert_to_tensor(mask_array)\n",
        "            mask_windows = window_partition(mask_array, self.window_size)\n",
        "            mask_windows = ops.reshape(mask_windows, [-1, self.window_size * self.window_size])\n",
        "            attn_mask = ops.expand_dims(mask_windows, axis=1) - ops.expand_dims(mask_windows, axis=2)\n",
        "            attn_mask = ops.where(attn_mask != 0, -100.0, attn_mask)\n",
        "            attn_mask = ops.where(attn_mask == 0, 0.0, attn_mask)\n",
        "            self.attn_mask = keras.Variable(initializer=attn_mask, shape=attn_mask.shape, dtype=attn_mask.dtype, trainable=False)\n",
        "        else:\n",
        "            self.attn_mask = None\n",
        "\n",
        "    def call(self, x, training=False):\n",
        "        height, width = self.num_patch\n",
        "        _, num_patches_before, channels = x.shape\n",
        "        x_skip = x\n",
        "        x = self.norm1(x)\n",
        "        x = ops.reshape(x, (-1, height, width, channels))\n",
        "        if self.shift_size > 0:\n",
        "            shifted_x = ops.roll(x, shift=[-self.shift_size, -self.shift_size], axis=[1, 2])\n",
        "        else:\n",
        "            shifted_x = x\n",
        "        x_windows = window_partition(shifted_x, self.window_size)\n",
        "        x_windows = ops.reshape(x_windows, (-1, self.window_size * self.window_size, channels))\n",
        "        attn_windows = self.attn(x_windows, mask=self.attn_mask)\n",
        "        attn_windows = ops.reshape(attn_windows, (-1, self.window_size, self.window_size, channels))\n",
        "        shifted_x = window_reverse(attn_windows, self.window_size, height, width, channels)\n",
        "        if self.shift_size > 0:\n",
        "            x = ops.roll(shifted_x, shift=[self.shift_size, self.shift_size], axis=[1, 2])\n",
        "        else:\n",
        "            x = shifted_x\n",
        "        x = ops.reshape(x, (-1, height * width, channels))\n",
        "        x = self.drop_path(x, training=training)\n",
        "        x = x_skip + x\n",
        "        x_skip = x\n",
        "        x = self.norm2(x)\n",
        "        x = self.mlp(x)\n",
        "        x = self.drop_path(x)\n",
        "        x = x_skip + x\n",
        "        return x\n",
        "\n",
        "class PatchEmbedding(layers.Layer):\n",
        "    def __init__(self, num_patch, embed_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.num_patch = num_patch\n",
        "        self.proj = layers.Dense(embed_dim)\n",
        "        self.pos_embed = layers.Embedding(input_dim=num_patch, output_dim=embed_dim)\n",
        "\n",
        "    def call(self, patch):\n",
        "        pos = ops.arange(start=0, stop=self.num_patch)\n",
        "        return self.proj(patch) + self.pos_embed(pos)\n",
        "\n",
        "class PatchMerging(keras.layers.Layer):\n",
        "    def __init__(self, num_patch, embed_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.num_patch = num_patch\n",
        "        self.embed_dim = embed_dim\n",
        "        self.linear_trans = layers.Dense(2 * embed_dim, use_bias=False)\n",
        "\n",
        "    def call(self, x):\n",
        "        height, width = self.num_patch\n",
        "        _, _, C = x.shape\n",
        "        x = ops.reshape(x, (-1, height, width, C))\n",
        "        x0 = x[:, 0::2, 0::2, :]\n",
        "        x1 = x[:, 1::2, 0::2, :]\n",
        "        x2 = x[:, 0::2, 1::2, :]\n",
        "        x3 = x[:, 1::2, 1::2, :]\n",
        "        x = ops.concatenate((x0, x1, x2, x3), axis=-1)\n",
        "        x = ops.reshape(x, (-1, (height // 2) * (width // 2), 4 * C))\n",
        "        return self.linear_trans(x)"
      ],
      "metadata": {
        "id": "y4ASxVmG39GD"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Assembly\n",
        "\n",
        "Now I will assemble the layers together to build the final model. For this first attempt, I'll use a simple structure with two SwinTransformer blocks followed by a patch merging layer and a final classification head."
      ],
      "metadata": {
        "id": "Edt6rS4e4HKY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Model Assembly ---\n",
        "patch_feature_dim = patch_size[0] * patch_size[1] * input_shape[2]\n",
        "num_patches_total = num_patch_x * num_patch_y\n",
        "input_layer_shape = (num_patches_total, patch_feature_dim)\n",
        "\n",
        "input_tensor = layers.Input(shape=input_layer_shape)\n",
        "\n",
        "# Stage 1\n",
        "x = PatchEmbedding(num_patches_total, embed_dim)(input_tensor)\n",
        "x = SwinTransformer(\n",
        "    dim=embed_dim,\n",
        "    num_patch=(num_patch_x, num_patch_y),\n",
        "    num_heads=num_heads,\n",
        "    window_size=window_size,\n",
        "    shift_size=0,\n",
        "    num_mlp=num_mlp,\n",
        "    name=\"swin_stage1_block1\"\n",
        ")(x)\n",
        "x = SwinTransformer(\n",
        "    dim=embed_dim,\n",
        "    num_patch=(num_patch_x, num_patch_y),\n",
        "    num_heads=num_heads,\n",
        "    window_size=window_size,\n",
        "    shift_size=shift_size,\n",
        "    num_mlp=num_mlp,\n",
        "    name=\"swin_stage1_block2\"\n",
        ")(x)\n",
        "\n",
        "# Downsampling\n",
        "x = PatchMerging((num_patch_x, num_patch_y), embed_dim=embed_dim)(x)\n",
        "\n",
        "# Final Classification Head\n",
        "x = layers.LayerNormalization(epsilon=1e-5)(x)\n",
        "x = layers.GlobalAveragePooling1D()(x)\n",
        "output = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
        "\n",
        "# Create the final model\n",
        "model = keras.Model(input_tensor, output)\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 475
        },
        "id": "GK78ksYh4HZG",
        "outputId": "d006c027-f07c-4a7b-8d46-c48122bc1e09"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_2\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_2\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3136\u001b[0m, \u001b[38;5;34m48\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ patch_embedding                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3136\u001b[0m, \u001b[38;5;34m96\u001b[0m)       │       \u001b[38;5;34m305,760\u001b[0m │\n",
              "│ (\u001b[38;5;33mPatchEmbedding\u001b[0m)                │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ swin_stage1_block1              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3136\u001b[0m, \u001b[38;5;34m96\u001b[0m)       │       \u001b[38;5;34m115,255\u001b[0m │\n",
              "│ (\u001b[38;5;33mSwinTransformer\u001b[0m)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ swin_stage1_block2              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3136\u001b[0m, \u001b[38;5;34m96\u001b[0m)       │       \u001b[38;5;34m268,919\u001b[0m │\n",
              "│ (\u001b[38;5;33mSwinTransformer\u001b[0m)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ patch_merging (\u001b[38;5;33mPatchMerging\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m, \u001b[38;5;34m192\u001b[0m)       │        \u001b[38;5;34m73,728\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ layer_normalization_4           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m, \u001b[38;5;34m192\u001b[0m)       │           \u001b[38;5;34m384\u001b[0m │\n",
              "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling1d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_10 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)              │           \u001b[38;5;34m965\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3136</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ patch_embedding                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3136</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">305,760</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PatchEmbedding</span>)                │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ swin_stage1_block1              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3136</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">115,255</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SwinTransformer</span>)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ swin_stage1_block2              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3136</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">268,919</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SwinTransformer</span>)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ patch_merging (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PatchMerging</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,728</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ layer_normalization_4           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>)       │           <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling1d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">965</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m765,011\u001b[0m (3.52 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">765,011</span> (3.52 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m606,545\u001b[0m (2.31 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">606,545</span> (2.31 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m158,466\u001b[0m (1.21 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">158,466</span> (1.21 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 2: Data Loading and Initial Training Attempt\n",
        "\n",
        "With the model architecture defined, the next step is to load the pre-processed data and create the `tf.data` pipeline. This involves loading the NumPy arrays we saved earlier and setting up the data pipeline for training.\n",
        "\n",
        "After that, I will compile the model and run the first training experiment to get a baseline performance."
      ],
      "metadata": {
        "id": "tefsthPoBDwp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Training Hyperparameters ---\n",
        "learning_rate = 1e-3\n",
        "batch_size = 64\n",
        "num_epochs = 150\n",
        "weight_decay = 0.0001\n",
        "label_smoothing = 0.1\n",
        "\n",
        "# --- Load Pre-processed Data ---\n",
        "# Note: This assumes Google Drive is mounted.\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    DRIVE_LOAD_DIR = \"/content/drive/MyDrive/Colab_Datasets/APTOS_Processed_Augmented\"\n",
        "    x_train = np.load(os.path.join(DRIVE_LOAD_DIR, 'X_train.npy'))\n",
        "    y_train = np.load(os.path.join(DRIVE_LOAD_DIR, 'y_train_one_hot.npy'))\n",
        "    # For now, I will create a validation set from the training data for a quick check\n",
        "    print(f\"Loaded x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading data. Make sure you've run the preprocessing notebook. Details: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3gjR9Oz6BEdx",
        "outputId": "d1005cf5-4cdf-4a9d-ef13-9512d8c69cfa"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Loaded x_train shape: (8790, 224, 224, 3) - y_train shape: (8790, 5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create `tf.data` Pipeline\n",
        "\n",
        "I'm creating the data pipeline. One important step here is extracting patches from the images, because the Swin Transformer doesn't work on whole images directly. The `patch_extract_wrapper` function handles this."
      ],
      "metadata": {
        "id": "B1_gQH7hBLCY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This function extracts patches from images, a required step for the model\n",
        "def patch_extract_wrapper(images):\n",
        "    batch_size = tf.shape(images)[0]\n",
        "    patches = tf.image.extract_patches(\n",
        "        images=images,\n",
        "        sizes=(1, patch_size[0], patch_size[1], 1),\n",
        "        strides=(1, patch_size[0], patch_size[1], 1),\n",
        "        rates=(1, 1, 1, 1),\n",
        "        padding=\"VALID\",\n",
        "    )\n",
        "    patch_dim = patches.shape[-1]\n",
        "    patch_num = patches.shape[1]\n",
        "    return tf.reshape(patches, (batch_size, patch_num * patch_num, patch_dim))\n",
        "\n",
        "# Creating a validation split from the training data for this run\n",
        "val_split_index = int(len(x_train) * 0.9)\n",
        "x_val_split = x_train[val_split_index:]\n",
        "y_val_split = y_train[val_split_index:]\n",
        "x_train_split = x_train[:val_split_index]\n",
        "y_train_split = y_train[:val_split_index]\n",
        "\n",
        "\n",
        "# Create the training dataset pipeline\n",
        "dataset = (\n",
        "    tf.data.Dataset.from_tensor_slices((x_train_split, y_train_split))\n",
        "    .batch(batch_size=batch_size)\n",
        "    .map(lambda x, y: (patch_extract_wrapper(x), y), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    .prefetch(tf.data.AUTOTUNE)\n",
        ")\n",
        "\n",
        "# Create the validation dataset pipeline\n",
        "dataset_val = (\n",
        "    tf.data.Dataset.from_tensor_slices((x_val_split, y_val_split))\n",
        "    .batch(batch_size=batch_size)\n",
        "    .map(lambda x, y: (patch_extract_wrapper(x), y), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    .prefetch(tf.data.AUTOTUNE)\n",
        ")\n",
        "\n",
        "print(\"Training and validation tf.data pipelines created.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iuTZ4boQBL6F",
        "outputId": "d4e0bfec-abae-43dd-8d5a-c7c26d1fa425"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training and validation tf.data pipelines created.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compile and Train the Model\n",
        "\n",
        "Now, everything is ready. I'll compile the model with the AdamW optimizer and categorical cross-entropy loss. Then, I will start the first training run."
      ],
      "metadata": {
        "id": "lZazOKOMBclx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Update: Resolving a Memory Error\n",
        "\n",
        "After running the initial setup, the training failed due to a `ResourceExhaustedError`. Even with Google Colab Pro, the `batch_size` of 128 proved to be too demanding for the GPU. To fix this, I'm reducing the batch size to a more reasonable value of 64. I will now restart the training."
      ],
      "metadata": {
        "id": "xcQFS4gFdWzv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "model.compile(\n",
        "    loss=keras.losses.CategoricalCrossentropy(label_smoothing=label_smoothing),\n",
        "    optimizer=keras.optimizers.AdamW(\n",
        "        learning_rate=learning_rate, weight_decay=weight_decay\n",
        "    ),\n",
        "    metrics=[\n",
        "        keras.metrics.CategoricalAccuracy(name=\"accuracy\"),\n",
        "        keras.metrics.TopKCategoricalAccuracy(2, name=\"top-2-accuracy\"),\n",
        "    ],\n",
        ")\n",
        "\n",
        "# Start training\n",
        "print(\"\\nStarting the first training experiment...\")\n",
        "history = model.fit(\n",
        "    dataset,\n",
        "    batch_size=batch_size,\n",
        "    epochs=num_epochs,\n",
        "    validation_data=dataset_val,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "688BymVPBdQY",
        "outputId": "4b9375de-578c-4a76-e00d-d9c893325244"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting the first training experiment...\n",
            "Epoch 1/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 394ms/step - accuracy: 0.4366 - loss: 1.6350 - top-2-accuracy: 0.6984 - val_accuracy: 0.6564 - val_loss: 1.1385 - val_top-2-accuracy: 0.8180\n",
            "Epoch 2/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 287ms/step - accuracy: 0.6291 - loss: 1.1702 - top-2-accuracy: 0.7836 - val_accuracy: 0.6655 - val_loss: 1.0978 - val_top-2-accuracy: 0.8191\n",
            "Epoch 3/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 277ms/step - accuracy: 0.6552 - loss: 1.1073 - top-2-accuracy: 0.8119 - val_accuracy: 0.6803 - val_loss: 1.0738 - val_top-2-accuracy: 0.8237\n",
            "Epoch 4/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.6696 - loss: 1.0780 - top-2-accuracy: 0.8221 - val_accuracy: 0.6860 - val_loss: 1.0707 - val_top-2-accuracy: 0.8225\n",
            "Epoch 5/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 281ms/step - accuracy: 0.6724 - loss: 1.0722 - top-2-accuracy: 0.8246 - val_accuracy: 0.6758 - val_loss: 1.0601 - val_top-2-accuracy: 0.8259\n",
            "Epoch 6/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 281ms/step - accuracy: 0.6764 - loss: 1.0630 - top-2-accuracy: 0.8218 - val_accuracy: 0.6860 - val_loss: 1.0538 - val_top-2-accuracy: 0.8259\n",
            "Epoch 7/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.6830 - loss: 1.0470 - top-2-accuracy: 0.8290 - val_accuracy: 0.6792 - val_loss: 1.0446 - val_top-2-accuracy: 0.8294\n",
            "Epoch 8/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 281ms/step - accuracy: 0.6915 - loss: 1.0287 - top-2-accuracy: 0.8374 - val_accuracy: 0.6940 - val_loss: 1.0384 - val_top-2-accuracy: 0.8328\n",
            "Epoch 9/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 281ms/step - accuracy: 0.6967 - loss: 1.0177 - top-2-accuracy: 0.8430 - val_accuracy: 0.6837 - val_loss: 1.0452 - val_top-2-accuracy: 0.8373\n",
            "Epoch 10/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.6982 - loss: 1.0087 - top-2-accuracy: 0.8467 - val_accuracy: 0.6792 - val_loss: 1.0556 - val_top-2-accuracy: 0.8294\n",
            "Epoch 11/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.7086 - loss: 0.9972 - top-2-accuracy: 0.8511 - val_accuracy: 0.6883 - val_loss: 1.0424 - val_top-2-accuracy: 0.8350\n",
            "Epoch 12/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 281ms/step - accuracy: 0.7114 - loss: 0.9938 - top-2-accuracy: 0.8470 - val_accuracy: 0.6894 - val_loss: 1.0425 - val_top-2-accuracy: 0.8294\n",
            "Epoch 13/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 281ms/step - accuracy: 0.7090 - loss: 0.9937 - top-2-accuracy: 0.8504 - val_accuracy: 0.6803 - val_loss: 1.0589 - val_top-2-accuracy: 0.8191\n",
            "Epoch 14/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 281ms/step - accuracy: 0.7119 - loss: 0.9895 - top-2-accuracy: 0.8523 - val_accuracy: 0.6849 - val_loss: 1.0454 - val_top-2-accuracy: 0.8328\n",
            "Epoch 15/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.7161 - loss: 0.9845 - top-2-accuracy: 0.8534 - val_accuracy: 0.6860 - val_loss: 1.0410 - val_top-2-accuracy: 0.8305\n",
            "Epoch 16/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.7163 - loss: 0.9761 - top-2-accuracy: 0.8584 - val_accuracy: 0.6837 - val_loss: 1.0447 - val_top-2-accuracy: 0.8328\n",
            "Epoch 17/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 281ms/step - accuracy: 0.7193 - loss: 0.9682 - top-2-accuracy: 0.8580 - val_accuracy: 0.6997 - val_loss: 1.0331 - val_top-2-accuracy: 0.8385\n",
            "Epoch 18/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 281ms/step - accuracy: 0.7235 - loss: 0.9595 - top-2-accuracy: 0.8591 - val_accuracy: 0.6746 - val_loss: 1.0409 - val_top-2-accuracy: 0.8294\n",
            "Epoch 19/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 281ms/step - accuracy: 0.7224 - loss: 0.9598 - top-2-accuracy: 0.8597 - val_accuracy: 0.6758 - val_loss: 1.0391 - val_top-2-accuracy: 0.8316\n",
            "Epoch 20/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 281ms/step - accuracy: 0.7305 - loss: 0.9500 - top-2-accuracy: 0.8636 - val_accuracy: 0.6860 - val_loss: 1.0480 - val_top-2-accuracy: 0.8385\n",
            "Epoch 21/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.7289 - loss: 0.9420 - top-2-accuracy: 0.8665 - val_accuracy: 0.6906 - val_loss: 1.0432 - val_top-2-accuracy: 0.8373\n",
            "Epoch 22/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.7346 - loss: 0.9372 - top-2-accuracy: 0.8619 - val_accuracy: 0.6849 - val_loss: 1.0470 - val_top-2-accuracy: 0.8328\n",
            "Epoch 23/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.7329 - loss: 0.9317 - top-2-accuracy: 0.8682 - val_accuracy: 0.6871 - val_loss: 1.0387 - val_top-2-accuracy: 0.8373\n",
            "Epoch 24/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.7359 - loss: 0.9308 - top-2-accuracy: 0.8707 - val_accuracy: 0.6780 - val_loss: 1.0634 - val_top-2-accuracy: 0.8362\n",
            "Epoch 25/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.7449 - loss: 0.9191 - top-2-accuracy: 0.8729 - val_accuracy: 0.6780 - val_loss: 1.0607 - val_top-2-accuracy: 0.8203\n",
            "Epoch 26/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 281ms/step - accuracy: 0.7527 - loss: 0.9109 - top-2-accuracy: 0.8742 - val_accuracy: 0.6712 - val_loss: 1.0577 - val_top-2-accuracy: 0.8248\n",
            "Epoch 27/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 281ms/step - accuracy: 0.7512 - loss: 0.9101 - top-2-accuracy: 0.8765 - val_accuracy: 0.6724 - val_loss: 1.0806 - val_top-2-accuracy: 0.8180\n",
            "Epoch 28/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 281ms/step - accuracy: 0.7519 - loss: 0.9093 - top-2-accuracy: 0.8769 - val_accuracy: 0.6906 - val_loss: 1.0686 - val_top-2-accuracy: 0.8339\n",
            "Epoch 29/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.7514 - loss: 0.9144 - top-2-accuracy: 0.8769 - val_accuracy: 0.6780 - val_loss: 1.0826 - val_top-2-accuracy: 0.8157\n",
            "Epoch 30/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.7510 - loss: 0.9076 - top-2-accuracy: 0.8814 - val_accuracy: 0.6780 - val_loss: 1.0741 - val_top-2-accuracy: 0.8180\n",
            "Epoch 31/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 281ms/step - accuracy: 0.7565 - loss: 0.8953 - top-2-accuracy: 0.8838 - val_accuracy: 0.6849 - val_loss: 1.0767 - val_top-2-accuracy: 0.8146\n",
            "Epoch 32/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 281ms/step - accuracy: 0.7633 - loss: 0.8947 - top-2-accuracy: 0.8863 - val_accuracy: 0.6917 - val_loss: 1.0690 - val_top-2-accuracy: 0.8282\n",
            "Epoch 33/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.7633 - loss: 0.8918 - top-2-accuracy: 0.8860 - val_accuracy: 0.6701 - val_loss: 1.0927 - val_top-2-accuracy: 0.8066\n",
            "Epoch 34/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.7669 - loss: 0.8853 - top-2-accuracy: 0.8861 - val_accuracy: 0.6792 - val_loss: 1.0749 - val_top-2-accuracy: 0.8077\n",
            "Epoch 35/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.7688 - loss: 0.8855 - top-2-accuracy: 0.8854 - val_accuracy: 0.6826 - val_loss: 1.0791 - val_top-2-accuracy: 0.8214\n",
            "Epoch 36/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.7701 - loss: 0.8800 - top-2-accuracy: 0.8876 - val_accuracy: 0.6917 - val_loss: 1.0806 - val_top-2-accuracy: 0.8191\n",
            "Epoch 37/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.7674 - loss: 0.8755 - top-2-accuracy: 0.8938 - val_accuracy: 0.6803 - val_loss: 1.0785 - val_top-2-accuracy: 0.8134\n",
            "Epoch 38/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 281ms/step - accuracy: 0.7703 - loss: 0.8706 - top-2-accuracy: 0.8989 - val_accuracy: 0.6894 - val_loss: 1.0832 - val_top-2-accuracy: 0.8180\n",
            "Epoch 39/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.7782 - loss: 0.8636 - top-2-accuracy: 0.8979 - val_accuracy: 0.6712 - val_loss: 1.1009 - val_top-2-accuracy: 0.8146\n",
            "Epoch 40/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.7815 - loss: 0.8583 - top-2-accuracy: 0.8972 - val_accuracy: 0.6758 - val_loss: 1.1044 - val_top-2-accuracy: 0.8077\n",
            "Epoch 41/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 281ms/step - accuracy: 0.7858 - loss: 0.8528 - top-2-accuracy: 0.8990 - val_accuracy: 0.6769 - val_loss: 1.1153 - val_top-2-accuracy: 0.8180\n",
            "Epoch 42/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.7842 - loss: 0.8548 - top-2-accuracy: 0.8964 - val_accuracy: 0.6871 - val_loss: 1.0992 - val_top-2-accuracy: 0.8282\n",
            "Epoch 43/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.7815 - loss: 0.8532 - top-2-accuracy: 0.8982 - val_accuracy: 0.6849 - val_loss: 1.0777 - val_top-2-accuracy: 0.8146\n",
            "Epoch 44/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.7780 - loss: 0.8568 - top-2-accuracy: 0.8984 - val_accuracy: 0.6735 - val_loss: 1.1049 - val_top-2-accuracy: 0.8055\n",
            "Epoch 45/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.7817 - loss: 0.8475 - top-2-accuracy: 0.9027 - val_accuracy: 0.6746 - val_loss: 1.0956 - val_top-2-accuracy: 0.8123\n",
            "Epoch 46/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 281ms/step - accuracy: 0.7884 - loss: 0.8408 - top-2-accuracy: 0.8999 - val_accuracy: 0.6815 - val_loss: 1.0857 - val_top-2-accuracy: 0.8066\n",
            "Epoch 47/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.7824 - loss: 0.8462 - top-2-accuracy: 0.9003 - val_accuracy: 0.6815 - val_loss: 1.0915 - val_top-2-accuracy: 0.8168\n",
            "Epoch 48/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.7912 - loss: 0.8267 - top-2-accuracy: 0.9077 - val_accuracy: 0.6769 - val_loss: 1.1309 - val_top-2-accuracy: 0.8043\n",
            "Epoch 49/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.7942 - loss: 0.8323 - top-2-accuracy: 0.8983 - val_accuracy: 0.6906 - val_loss: 1.0823 - val_top-2-accuracy: 0.8294\n",
            "Epoch 50/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 281ms/step - accuracy: 0.7936 - loss: 0.8289 - top-2-accuracy: 0.9072 - val_accuracy: 0.6871 - val_loss: 1.1010 - val_top-2-accuracy: 0.8225\n",
            "Epoch 51/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.7986 - loss: 0.8171 - top-2-accuracy: 0.9109 - val_accuracy: 0.6815 - val_loss: 1.0948 - val_top-2-accuracy: 0.8146\n",
            "Epoch 52/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 281ms/step - accuracy: 0.8076 - loss: 0.8065 - top-2-accuracy: 0.9151 - val_accuracy: 0.6803 - val_loss: 1.0970 - val_top-2-accuracy: 0.8214\n",
            "Epoch 53/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 281ms/step - accuracy: 0.8062 - loss: 0.8080 - top-2-accuracy: 0.9099 - val_accuracy: 0.6871 - val_loss: 1.0851 - val_top-2-accuracy: 0.8305\n",
            "Epoch 54/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.8011 - loss: 0.8173 - top-2-accuracy: 0.9095 - val_accuracy: 0.6689 - val_loss: 1.1181 - val_top-2-accuracy: 0.8111\n",
            "Epoch 55/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.8044 - loss: 0.8056 - top-2-accuracy: 0.9132 - val_accuracy: 0.6735 - val_loss: 1.1105 - val_top-2-accuracy: 0.8191\n",
            "Epoch 56/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.7985 - loss: 0.8096 - top-2-accuracy: 0.9128 - val_accuracy: 0.6644 - val_loss: 1.1207 - val_top-2-accuracy: 0.8032\n",
            "Epoch 57/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.8064 - loss: 0.8016 - top-2-accuracy: 0.9152 - val_accuracy: 0.6871 - val_loss: 1.1009 - val_top-2-accuracy: 0.8134\n",
            "Epoch 58/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.8078 - loss: 0.7974 - top-2-accuracy: 0.9213 - val_accuracy: 0.6735 - val_loss: 1.1386 - val_top-2-accuracy: 0.8032\n",
            "Epoch 59/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 281ms/step - accuracy: 0.8147 - loss: 0.7870 - top-2-accuracy: 0.9211 - val_accuracy: 0.6542 - val_loss: 1.1349 - val_top-2-accuracy: 0.8009\n",
            "Epoch 60/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.8135 - loss: 0.7903 - top-2-accuracy: 0.9215 - val_accuracy: 0.6746 - val_loss: 1.1267 - val_top-2-accuracy: 0.8123\n",
            "Epoch 61/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.8156 - loss: 0.7868 - top-2-accuracy: 0.9215 - val_accuracy: 0.6712 - val_loss: 1.1244 - val_top-2-accuracy: 0.8089\n",
            "Epoch 62/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.8301 - loss: 0.7708 - top-2-accuracy: 0.9275 - val_accuracy: 0.6860 - val_loss: 1.0968 - val_top-2-accuracy: 0.8203\n",
            "Epoch 63/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.8290 - loss: 0.7639 - top-2-accuracy: 0.9301 - val_accuracy: 0.6917 - val_loss: 1.0943 - val_top-2-accuracy: 0.8203\n",
            "Epoch 64/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 281ms/step - accuracy: 0.8324 - loss: 0.7587 - top-2-accuracy: 0.9313 - val_accuracy: 0.6917 - val_loss: 1.1053 - val_top-2-accuracy: 0.8305\n",
            "Epoch 65/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.8385 - loss: 0.7452 - top-2-accuracy: 0.9378 - val_accuracy: 0.6735 - val_loss: 1.1024 - val_top-2-accuracy: 0.8066\n",
            "Epoch 66/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.8370 - loss: 0.7509 - top-2-accuracy: 0.9351 - val_accuracy: 0.6655 - val_loss: 1.1121 - val_top-2-accuracy: 0.8066\n",
            "Epoch 67/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.8448 - loss: 0.7444 - top-2-accuracy: 0.9352 - val_accuracy: 0.6837 - val_loss: 1.1092 - val_top-2-accuracy: 0.8157\n",
            "Epoch 68/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.8448 - loss: 0.7367 - top-2-accuracy: 0.9393 - val_accuracy: 0.6803 - val_loss: 1.1205 - val_top-2-accuracy: 0.8123\n",
            "Epoch 69/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.8472 - loss: 0.7306 - top-2-accuracy: 0.9370 - val_accuracy: 0.6724 - val_loss: 1.1434 - val_top-2-accuracy: 0.8089\n",
            "Epoch 70/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.8486 - loss: 0.7323 - top-2-accuracy: 0.9422 - val_accuracy: 0.6826 - val_loss: 1.1515 - val_top-2-accuracy: 0.8032\n",
            "Epoch 71/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.8514 - loss: 0.7228 - top-2-accuracy: 0.9429 - val_accuracy: 0.6621 - val_loss: 1.1612 - val_top-2-accuracy: 0.8168\n",
            "Epoch 72/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.8548 - loss: 0.7249 - top-2-accuracy: 0.9448 - val_accuracy: 0.6667 - val_loss: 1.2068 - val_top-2-accuracy: 0.8214\n",
            "Epoch 73/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.8565 - loss: 0.7243 - top-2-accuracy: 0.9455 - val_accuracy: 0.6792 - val_loss: 1.1551 - val_top-2-accuracy: 0.8191\n",
            "Epoch 74/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 281ms/step - accuracy: 0.8646 - loss: 0.7058 - top-2-accuracy: 0.9495 - val_accuracy: 0.6598 - val_loss: 1.2064 - val_top-2-accuracy: 0.8146\n",
            "Epoch 75/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 281ms/step - accuracy: 0.8641 - loss: 0.7065 - top-2-accuracy: 0.9508 - val_accuracy: 0.6610 - val_loss: 1.1916 - val_top-2-accuracy: 0.7941\n",
            "Epoch 76/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.8596 - loss: 0.7139 - top-2-accuracy: 0.9463 - val_accuracy: 0.6644 - val_loss: 1.1940 - val_top-2-accuracy: 0.8111\n",
            "Epoch 77/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.8685 - loss: 0.6970 - top-2-accuracy: 0.9498 - val_accuracy: 0.6735 - val_loss: 1.2004 - val_top-2-accuracy: 0.8009\n",
            "Epoch 78/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.8766 - loss: 0.6900 - top-2-accuracy: 0.9514 - val_accuracy: 0.6576 - val_loss: 1.1876 - val_top-2-accuracy: 0.8009\n",
            "Epoch 79/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.8751 - loss: 0.6884 - top-2-accuracy: 0.9531 - val_accuracy: 0.6542 - val_loss: 1.2178 - val_top-2-accuracy: 0.8111\n",
            "Epoch 80/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 281ms/step - accuracy: 0.8735 - loss: 0.6876 - top-2-accuracy: 0.9528 - val_accuracy: 0.6519 - val_loss: 1.2151 - val_top-2-accuracy: 0.7975\n",
            "Epoch 81/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.8754 - loss: 0.6843 - top-2-accuracy: 0.9571 - val_accuracy: 0.6724 - val_loss: 1.1958 - val_top-2-accuracy: 0.7998\n",
            "Epoch 82/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.8774 - loss: 0.6827 - top-2-accuracy: 0.9511 - val_accuracy: 0.6621 - val_loss: 1.1895 - val_top-2-accuracy: 0.8294\n",
            "Epoch 83/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.8845 - loss: 0.6698 - top-2-accuracy: 0.9600 - val_accuracy: 0.6724 - val_loss: 1.1740 - val_top-2-accuracy: 0.8089\n",
            "Epoch 84/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.8907 - loss: 0.6614 - top-2-accuracy: 0.9626 - val_accuracy: 0.6621 - val_loss: 1.2103 - val_top-2-accuracy: 0.8077\n",
            "Epoch 85/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.8819 - loss: 0.6657 - top-2-accuracy: 0.9606 - val_accuracy: 0.6758 - val_loss: 1.1740 - val_top-2-accuracy: 0.8146\n",
            "Epoch 86/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.8868 - loss: 0.6637 - top-2-accuracy: 0.9624 - val_accuracy: 0.6428 - val_loss: 1.2382 - val_top-2-accuracy: 0.7850\n",
            "Epoch 87/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.8852 - loss: 0.6636 - top-2-accuracy: 0.9604 - val_accuracy: 0.6428 - val_loss: 1.2387 - val_top-2-accuracy: 0.7895\n",
            "Epoch 88/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.8926 - loss: 0.6573 - top-2-accuracy: 0.9612 - val_accuracy: 0.6473 - val_loss: 1.2335 - val_top-2-accuracy: 0.7975\n",
            "Epoch 89/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.8964 - loss: 0.6474 - top-2-accuracy: 0.9640 - val_accuracy: 0.6553 - val_loss: 1.2321 - val_top-2-accuracy: 0.8066\n",
            "Epoch 90/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.8970 - loss: 0.6448 - top-2-accuracy: 0.9650 - val_accuracy: 0.6416 - val_loss: 1.2561 - val_top-2-accuracy: 0.8077\n",
            "Epoch 91/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.8840 - loss: 0.6568 - top-2-accuracy: 0.9666 - val_accuracy: 0.6280 - val_loss: 1.2203 - val_top-2-accuracy: 0.7850\n",
            "Epoch 92/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.8875 - loss: 0.6593 - top-2-accuracy: 0.9642 - val_accuracy: 0.6553 - val_loss: 1.2005 - val_top-2-accuracy: 0.7952\n",
            "Epoch 93/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.9023 - loss: 0.6358 - top-2-accuracy: 0.9711 - val_accuracy: 0.6451 - val_loss: 1.2484 - val_top-2-accuracy: 0.7941\n",
            "Epoch 94/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.8992 - loss: 0.6334 - top-2-accuracy: 0.9665 - val_accuracy: 0.6564 - val_loss: 1.2180 - val_top-2-accuracy: 0.8032\n",
            "Epoch 95/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.9030 - loss: 0.6230 - top-2-accuracy: 0.9749 - val_accuracy: 0.6485 - val_loss: 1.2483 - val_top-2-accuracy: 0.8134\n",
            "Epoch 96/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.9117 - loss: 0.6257 - top-2-accuracy: 0.9724 - val_accuracy: 0.6667 - val_loss: 1.2101 - val_top-2-accuracy: 0.7986\n",
            "Epoch 97/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 283ms/step - accuracy: 0.9028 - loss: 0.6281 - top-2-accuracy: 0.9723 - val_accuracy: 0.6530 - val_loss: 1.2521 - val_top-2-accuracy: 0.7941\n",
            "Epoch 98/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.9123 - loss: 0.6190 - top-2-accuracy: 0.9758 - val_accuracy: 0.6519 - val_loss: 1.2386 - val_top-2-accuracy: 0.7918\n",
            "Epoch 99/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.9158 - loss: 0.6076 - top-2-accuracy: 0.9788 - val_accuracy: 0.6428 - val_loss: 1.2653 - val_top-2-accuracy: 0.7804\n",
            "Epoch 100/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.9152 - loss: 0.6104 - top-2-accuracy: 0.9759 - val_accuracy: 0.6485 - val_loss: 1.2804 - val_top-2-accuracy: 0.7759\n",
            "Epoch 101/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.9184 - loss: 0.6085 - top-2-accuracy: 0.9755 - val_accuracy: 0.6507 - val_loss: 1.2740 - val_top-2-accuracy: 0.7929\n",
            "Epoch 102/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.9193 - loss: 0.6038 - top-2-accuracy: 0.9793 - val_accuracy: 0.6610 - val_loss: 1.2482 - val_top-2-accuracy: 0.7986\n",
            "Epoch 103/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.9215 - loss: 0.5953 - top-2-accuracy: 0.9802 - val_accuracy: 0.6598 - val_loss: 1.2398 - val_top-2-accuracy: 0.7884\n",
            "Epoch 104/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.9230 - loss: 0.6002 - top-2-accuracy: 0.9801 - val_accuracy: 0.6496 - val_loss: 1.2548 - val_top-2-accuracy: 0.8043\n",
            "Epoch 105/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.9172 - loss: 0.6055 - top-2-accuracy: 0.9771 - val_accuracy: 0.6439 - val_loss: 1.2702 - val_top-2-accuracy: 0.7759\n",
            "Epoch 106/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.9274 - loss: 0.5940 - top-2-accuracy: 0.9792 - val_accuracy: 0.6462 - val_loss: 1.2635 - val_top-2-accuracy: 0.7998\n",
            "Epoch 107/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.9310 - loss: 0.5870 - top-2-accuracy: 0.9793 - val_accuracy: 0.6519 - val_loss: 1.2994 - val_top-2-accuracy: 0.8066\n",
            "Epoch 108/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.9335 - loss: 0.5789 - top-2-accuracy: 0.9841 - val_accuracy: 0.6553 - val_loss: 1.2553 - val_top-2-accuracy: 0.8055\n",
            "Epoch 109/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.9322 - loss: 0.5775 - top-2-accuracy: 0.9849 - val_accuracy: 0.6598 - val_loss: 1.2807 - val_top-2-accuracy: 0.8134\n",
            "Epoch 110/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 281ms/step - accuracy: 0.9275 - loss: 0.5860 - top-2-accuracy: 0.9832 - val_accuracy: 0.6473 - val_loss: 1.2985 - val_top-2-accuracy: 0.7952\n",
            "Epoch 111/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 281ms/step - accuracy: 0.9386 - loss: 0.5692 - top-2-accuracy: 0.9841 - val_accuracy: 0.6371 - val_loss: 1.2848 - val_top-2-accuracy: 0.7986\n",
            "Epoch 112/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.9422 - loss: 0.5702 - top-2-accuracy: 0.9852 - val_accuracy: 0.6530 - val_loss: 1.2630 - val_top-2-accuracy: 0.8009\n",
            "Epoch 113/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.9321 - loss: 0.5758 - top-2-accuracy: 0.9832 - val_accuracy: 0.6678 - val_loss: 1.2758 - val_top-2-accuracy: 0.8009\n",
            "Epoch 114/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.9354 - loss: 0.5758 - top-2-accuracy: 0.9841 - val_accuracy: 0.6655 - val_loss: 1.2937 - val_top-2-accuracy: 0.7998\n",
            "Epoch 115/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.9357 - loss: 0.5706 - top-2-accuracy: 0.9872 - val_accuracy: 0.6348 - val_loss: 1.3364 - val_top-2-accuracy: 0.8055\n",
            "Epoch 116/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.9390 - loss: 0.5685 - top-2-accuracy: 0.9862 - val_accuracy: 0.6246 - val_loss: 1.3037 - val_top-2-accuracy: 0.7850\n",
            "Epoch 117/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.9278 - loss: 0.5926 - top-2-accuracy: 0.9812 - val_accuracy: 0.6542 - val_loss: 1.2479 - val_top-2-accuracy: 0.7918\n",
            "Epoch 118/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.9326 - loss: 0.5840 - top-2-accuracy: 0.9852 - val_accuracy: 0.6655 - val_loss: 1.2505 - val_top-2-accuracy: 0.8123\n",
            "Epoch 119/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.9344 - loss: 0.5822 - top-2-accuracy: 0.9836 - val_accuracy: 0.6530 - val_loss: 1.2915 - val_top-2-accuracy: 0.8089\n",
            "Epoch 120/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.9347 - loss: 0.5731 - top-2-accuracy: 0.9855 - val_accuracy: 0.6621 - val_loss: 1.2856 - val_top-2-accuracy: 0.8020\n",
            "Epoch 121/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.9414 - loss: 0.5606 - top-2-accuracy: 0.9877 - val_accuracy: 0.6655 - val_loss: 1.2818 - val_top-2-accuracy: 0.8009\n",
            "Epoch 122/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.9449 - loss: 0.5561 - top-2-accuracy: 0.9878 - val_accuracy: 0.6746 - val_loss: 1.2614 - val_top-2-accuracy: 0.8157\n",
            "Epoch 123/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 281ms/step - accuracy: 0.9429 - loss: 0.5603 - top-2-accuracy: 0.9859 - val_accuracy: 0.6678 - val_loss: 1.2606 - val_top-2-accuracy: 0.8146\n",
            "Epoch 124/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.9447 - loss: 0.5594 - top-2-accuracy: 0.9874 - val_accuracy: 0.6701 - val_loss: 1.2636 - val_top-2-accuracy: 0.8123\n",
            "Epoch 125/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.9425 - loss: 0.5637 - top-2-accuracy: 0.9872 - val_accuracy: 0.6769 - val_loss: 1.2629 - val_top-2-accuracy: 0.8043\n",
            "Epoch 126/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.9423 - loss: 0.5585 - top-2-accuracy: 0.9867 - val_accuracy: 0.6439 - val_loss: 1.2966 - val_top-2-accuracy: 0.8020\n",
            "Epoch 127/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.9443 - loss: 0.5589 - top-2-accuracy: 0.9875 - val_accuracy: 0.6667 - val_loss: 1.3299 - val_top-2-accuracy: 0.8066\n",
            "Epoch 128/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.9455 - loss: 0.5587 - top-2-accuracy: 0.9866 - val_accuracy: 0.6542 - val_loss: 1.3239 - val_top-2-accuracy: 0.7793\n",
            "Epoch 129/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.9459 - loss: 0.5553 - top-2-accuracy: 0.9908 - val_accuracy: 0.6234 - val_loss: 1.3656 - val_top-2-accuracy: 0.7804\n",
            "Epoch 130/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.9518 - loss: 0.5461 - top-2-accuracy: 0.9887 - val_accuracy: 0.6268 - val_loss: 1.3039 - val_top-2-accuracy: 0.7850\n",
            "Epoch 131/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.9512 - loss: 0.5441 - top-2-accuracy: 0.9881 - val_accuracy: 0.6086 - val_loss: 1.4199 - val_top-2-accuracy: 0.7702\n",
            "Epoch 132/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 281ms/step - accuracy: 0.9537 - loss: 0.5449 - top-2-accuracy: 0.9892 - val_accuracy: 0.6189 - val_loss: 1.3349 - val_top-2-accuracy: 0.7861\n",
            "Epoch 133/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.9561 - loss: 0.5372 - top-2-accuracy: 0.9913 - val_accuracy: 0.6416 - val_loss: 1.3073 - val_top-2-accuracy: 0.7873\n",
            "Epoch 134/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.9501 - loss: 0.5468 - top-2-accuracy: 0.9898 - val_accuracy: 0.6530 - val_loss: 1.2826 - val_top-2-accuracy: 0.8032\n",
            "Epoch 135/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 281ms/step - accuracy: 0.9559 - loss: 0.5381 - top-2-accuracy: 0.9888 - val_accuracy: 0.6382 - val_loss: 1.2860 - val_top-2-accuracy: 0.7884\n",
            "Epoch 136/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.9545 - loss: 0.5361 - top-2-accuracy: 0.9923 - val_accuracy: 0.6223 - val_loss: 1.3087 - val_top-2-accuracy: 0.7895\n",
            "Epoch 137/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.9603 - loss: 0.5319 - top-2-accuracy: 0.9930 - val_accuracy: 0.6212 - val_loss: 1.3415 - val_top-2-accuracy: 0.7793\n",
            "Epoch 138/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.9638 - loss: 0.5277 - top-2-accuracy: 0.9928 - val_accuracy: 0.6303 - val_loss: 1.3263 - val_top-2-accuracy: 0.7782\n",
            "Epoch 139/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.9630 - loss: 0.5225 - top-2-accuracy: 0.9934 - val_accuracy: 0.6359 - val_loss: 1.2855 - val_top-2-accuracy: 0.8066\n",
            "Epoch 140/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.9641 - loss: 0.5223 - top-2-accuracy: 0.9919 - val_accuracy: 0.6473 - val_loss: 1.3013 - val_top-2-accuracy: 0.7975\n",
            "Epoch 141/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 281ms/step - accuracy: 0.9629 - loss: 0.5228 - top-2-accuracy: 0.9919 - val_accuracy: 0.6325 - val_loss: 1.3345 - val_top-2-accuracy: 0.7998\n",
            "Epoch 142/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.9640 - loss: 0.5197 - top-2-accuracy: 0.9935 - val_accuracy: 0.6428 - val_loss: 1.3351 - val_top-2-accuracy: 0.7918\n",
            "Epoch 143/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.9632 - loss: 0.5209 - top-2-accuracy: 0.9928 - val_accuracy: 0.6405 - val_loss: 1.3464 - val_top-2-accuracy: 0.7884\n",
            "Epoch 144/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.9627 - loss: 0.5243 - top-2-accuracy: 0.9925 - val_accuracy: 0.6462 - val_loss: 1.3324 - val_top-2-accuracy: 0.7918\n",
            "Epoch 145/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.9624 - loss: 0.5232 - top-2-accuracy: 0.9941 - val_accuracy: 0.6337 - val_loss: 1.3493 - val_top-2-accuracy: 0.7952\n",
            "Epoch 146/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.9636 - loss: 0.5200 - top-2-accuracy: 0.9932 - val_accuracy: 0.6348 - val_loss: 1.3480 - val_top-2-accuracy: 0.8066\n",
            "Epoch 147/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.9668 - loss: 0.5145 - top-2-accuracy: 0.9940 - val_accuracy: 0.6473 - val_loss: 1.3351 - val_top-2-accuracy: 0.7895\n",
            "Epoch 148/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.9696 - loss: 0.5098 - top-2-accuracy: 0.9945 - val_accuracy: 0.6507 - val_loss: 1.3326 - val_top-2-accuracy: 0.7861\n",
            "Epoch 149/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.9692 - loss: 0.5106 - top-2-accuracy: 0.9945 - val_accuracy: 0.6359 - val_loss: 1.2971 - val_top-2-accuracy: 0.7929\n",
            "Epoch 150/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.9727 - loss: 0.5032 - top-2-accuracy: 0.9950 - val_accuracy: 0.6359 - val_loss: 1.3203 - val_top-2-accuracy: 0.7918\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 3: Analyzing the First Training Results\n",
        "\n",
        "The training has now completed, and the results are not ideal. As can be seen from the logs above, the training accuracy climbed to over 97%, but the validation accuracy stalled around 68% and then started to fluctuate or even decrease. Similarly, the validation loss began to increase after an initial drop.\n",
        "\n",
        "This is a clear indication of **severe overfiting**. The model has memorized the training set but failed to generalize to new data. For the next attempt, I need to implement a mechanism to stop the training process before this overfitting becomes too extreme."
      ],
      "metadata": {
        "id": "T3DfhAapWdcu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 4: Visualizing Performance and Adding Evaluation Metrics\n",
        "\n",
        "To get a clearer picture of the overfitting problem, I'll now visualize the training history. I'm also adding more detailed evaluation functions to generate classification reports and confusion matrices for future experiments."
      ],
      "metadata": {
        "id": "z9uEkSwImscF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "def plot_training_history(history):\n",
        "    # Function to plot the training and validation curves\n",
        "    acc = history.history['accuracy']\n",
        "    val_acc = history.history['val_accuracy']\n",
        "    loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "    epochs = range(1, len(acc) + 1)\n",
        "\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs, acc, 'bo-', label='Training Accuracy')\n",
        "    plt.plot(epochs, val_acc, 'ro-', label='Validation Accuracy')\n",
        "    plt.title('Training and Validation Accuracy')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs, loss, 'bo-', label='Training Loss')\n",
        "    plt.plot(epochs, val_loss, 'ro-', label='Validation Loss')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "# Plot the curves from the completed training run\n",
        "print(\"Plotting the training history from the first experiment...\")\n",
        "plot_training_history(history)\n",
        "\n",
        "\n",
        "\n",
        "def evaluate_model_on_dataset(model, dataset):\n",
        "    print(\"\\nGenerating predictions for evaluation...\")\n",
        "    y_pred_probs = model.predict(dataset)\n",
        "    y_pred_classes = np.argmax(y_pred_probs, axis=1)\n",
        "\n",
        "    y_true_classes = []\n",
        "    for _, labels in dataset:\n",
        "        y_true_classes.extend(np.argmax(labels.numpy(), axis=1))\n",
        "\n",
        "    y_true_classes = np.array(y_true_classes)\n",
        "\n",
        "    class_names = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferative']\n",
        "\n",
        "    # Classification Report\n",
        "    print(\"\\n--- Classification Report ---\")\n",
        "    print(classification_report(y_true_classes, y_pred_classes, target_names=class_names))\n",
        "\n",
        "    # Confusion Matrix\n",
        "    print(\"\\n--- Confusion Matrix ---\")\n",
        "    cm = confusion_matrix(y_true_classes, y_pred_classes)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.ylabel('Actual Label')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.show()\n",
        "\n",
        "print(\"Evaluation functions defined.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "t365FYT9Wcmo",
        "outputId": "adc85c26-f20b-4976-d5e5-6a6a560485af"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Plotting the training history from the first experiment...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'history' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-924870256.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m# Plot the curves from the completed training run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Plotting the training history from the first experiment...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mplot_training_history\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K7pItdOTnT6W"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}