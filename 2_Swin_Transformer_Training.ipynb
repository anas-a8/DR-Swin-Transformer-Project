{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sKCAHmIr3nQ1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 1: Building the Core Architecture\n",
        "\n",
        "This notebook covers the main part of the project: implementing the custom Swin Transformer model. The first step is to define all the custom layers needed, like PatchEmbedding, WindowAttention, and the main SwinTransformer block itself. This is the foundation of the model."
      ],
      "metadata": {
        "id": "Hy7m2rRX3w6g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras import layers\n",
        "from keras import ops\n",
        "import os\n",
        "\n",
        "# --- Model Hyperparameters ---\n",
        "num_classes = 5\n",
        "input_shape = (224, 224, 3)\n",
        "patch_size = (4, 4)\n",
        "num_heads = 6\n",
        "embed_dim = 96\n",
        "num_mlp = 384\n",
        "qkv_bias = True\n",
        "window_size = 7\n",
        "shift_size = 3\n",
        "image_dimension = 224\n",
        "\n",
        "# --- Calculate patch details ---\n",
        "num_patch_x = input_shape[0] // patch_size[0]\n",
        "num_patch_y = input_shape[1] // patch_size[1]\n",
        "print(f\"Number of patches: {num_patch_x}x{num_patch_y} = {num_patch_x * num_patch_y}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lz0faHNZ3xUR",
        "outputId": "e2aadb1b-405f-469f-9f5b-cc417fa72085"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of patches: 56x56 = 3136\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Helper Functions\n",
        "\n",
        "These two functions are used to partition the image into windows and then reverse the proccess. This is a core idea of the Swin Transformer to compute attention locally."
      ],
      "metadata": {
        "id": "XCG0d0m-35-X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def window_partition(x, window_size):\n",
        "    _, height, width, channels = x.shape\n",
        "    patch_num_y = height // window_size\n",
        "    patch_num_x = width // window_size\n",
        "    x = ops.reshape(\n",
        "        x, (-1, patch_num_y, window_size, patch_num_x, window_size, channels)\n",
        "    )\n",
        "    x = ops.transpose(x, (0, 1, 3, 2, 4, 5))\n",
        "    windows = ops.reshape(x, (-1, window_size, window_size, channels))\n",
        "    return windows\n",
        "\n",
        "def window_reverse(windows, window_size, height, width, channels):\n",
        "    patch_num_y = height // window_size\n",
        "    patch_num_x = width // window_size\n",
        "    x = ops.reshape(\n",
        "        windows, (-1, patch_num_y, patch_num_x, window_size, window_size, channels)\n",
        "    )\n",
        "    x = ops.transpose(x, (0, 1, 3, 2, 4, 5))\n",
        "    x = ops.reshape(x, (-1, height, width, channels))\n",
        "    return x"
      ],
      "metadata": {
        "id": "VfmdZXOy36Vm"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Core Layers Implementation\n",
        "\n",
        "Now, I'm defining the main custom layers for the model.\n",
        "\n",
        "1.  **WindowAttention**: Computes self-attention within local windows.\n",
        "2.  **SwinTransformer**: The main block that combines window attention with the MLP.\n",
        "3.  **PatchEmbedding**: Converts image patches to vector embeddings.\n",
        "4.  **PatchMerging**: Downsamples the image by merging patches, which creates the hirearchical structure."
      ],
      "metadata": {
        "id": "FoXAtMZ33_DP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class WindowAttention(layers.Layer):\n",
        "    def __init__(self, dim, window_size, num_heads, qkv_bias=True, dropout_rate=0.0, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.dim = dim\n",
        "        self.window_size = window_size\n",
        "        self.num_heads = num_heads\n",
        "        self.scale = (dim // num_heads) ** -0.5\n",
        "        self.qkv = layers.Dense(dim * 3, use_bias=qkv_bias)\n",
        "        self.dropout = layers.Dropout(dropout_rate)\n",
        "        self.proj = layers.Dense(dim)\n",
        "\n",
        "        # Define relative position bias\n",
        "        num_window_elements = (2 * self.window_size[0] - 1) * (2 * self.window_size[1] - 1)\n",
        "        self.relative_position_bias_table = self.add_weight(\n",
        "            shape=(num_window_elements, self.num_heads),\n",
        "            initializer=keras.initializers.Zeros(),\n",
        "            trainable=True,\n",
        "        )\n",
        "        coords_h = np.arange(self.window_size[0])\n",
        "        coords_w = np.arange(self.window_size[1])\n",
        "        coords_matrix = np.meshgrid(coords_h, coords_w, indexing=\"ij\")\n",
        "        coords = np.stack(coords_matrix)\n",
        "        coords_flatten = coords.reshape(2, -1)\n",
        "        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]\n",
        "        relative_coords = relative_coords.transpose([1, 2, 0])\n",
        "        relative_coords[:, :, 0] += self.window_size[0] - 1\n",
        "        relative_coords[:, :, 1] += self.window_size[1] - 1\n",
        "        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n",
        "        relative_position_index = relative_coords.sum(-1)\n",
        "        self.relative_position_index = keras.Variable(\n",
        "            initializer=relative_position_index,\n",
        "            shape=relative_position_index.shape,\n",
        "            dtype=\"int\",\n",
        "            trainable=False,\n",
        "        )\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        _, size, channels = x.shape\n",
        "        head_dim = channels // self.num_heads\n",
        "        x_qkv = self.qkv(x)\n",
        "        x_qkv = ops.reshape(x_qkv, (-1, size, 3, self.num_heads, head_dim))\n",
        "        x_qkv = ops.transpose(x_qkv, (2, 0, 3, 1, 4))\n",
        "        q, k, v = x_qkv[0], x_qkv[1], x_qkv[2]\n",
        "        q = q * self.scale\n",
        "        k = ops.transpose(k, (0, 1, 3, 2))\n",
        "        attn = q @ k\n",
        "\n",
        "        num_window_elements = self.window_size[0] * self.window_size[1]\n",
        "        relative_position_index_flat = ops.reshape(self.relative_position_index, (-1,))\n",
        "        relative_position_bias = ops.take(self.relative_position_bias_table, relative_position_index_flat, axis=0)\n",
        "        relative_position_bias = ops.reshape(relative_position_bias, (num_window_elements, num_window_elements, -1))\n",
        "        relative_position_bias = ops.transpose(relative_position_bias, (2, 0, 1))\n",
        "        attn = attn + ops.expand_dims(relative_position_bias, axis=0)\n",
        "\n",
        "        if mask is not None:\n",
        "            nW = mask.shape[0]\n",
        "            mask_float = ops.cast(ops.expand_dims(ops.expand_dims(mask, axis=1), axis=0), \"float32\")\n",
        "            attn = ops.reshape(attn, (-1, nW, self.num_heads, size, size)) + mask_float\n",
        "            attn = ops.reshape(attn, (-1, self.num_heads, size, size))\n",
        "            attn = keras.activations.softmax(attn, axis=-1)\n",
        "        else:\n",
        "            attn = keras.activations.softmax(attn, axis=-1)\n",
        "\n",
        "        attn = self.dropout(attn)\n",
        "        x_qkv = attn @ v\n",
        "        x_qkv = ops.transpose(x_qkv, (0, 2, 1, 3))\n",
        "        x_qkv = ops.reshape(x_qkv, (-1, size, channels))\n",
        "        x_qkv = self.proj(x_qkv)\n",
        "        x_qkv = self.dropout(x_qkv)\n",
        "        return x_qkv\n",
        "\n",
        "class SwinTransformer(layers.Layer):\n",
        "    def __init__(self, dim, num_patch, num_heads, window_size=7, shift_size=0, num_mlp=1024, qkv_bias=True, dropout_rate=0.0, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.dim = dim\n",
        "        self.num_patch = num_patch\n",
        "        self.num_heads = num_heads\n",
        "        self.window_size = window_size\n",
        "        self.shift_size = shift_size\n",
        "        self.num_mlp = num_mlp\n",
        "        self.norm1 = layers.LayerNormalization(epsilon=1e-5)\n",
        "        self.attn = WindowAttention(\n",
        "            dim,\n",
        "            window_size=(self.window_size, self.window_size),\n",
        "            num_heads=num_heads,\n",
        "            qkv_bias=qkv_bias,\n",
        "            dropout_rate=dropout_rate,\n",
        "        )\n",
        "        self.drop_path = layers.Dropout(dropout_rate)\n",
        "        self.norm2 = layers.LayerNormalization(epsilon=1e-5)\n",
        "        self.mlp = keras.Sequential([\n",
        "            layers.Dense(num_mlp),\n",
        "            layers.Activation(keras.activations.gelu),\n",
        "            layers.Dropout(dropout_rate),\n",
        "            layers.Dense(dim),\n",
        "            layers.Dropout(dropout_rate),\n",
        "        ])\n",
        "        if min(self.num_patch) < self.window_size:\n",
        "            self.shift_size = 0\n",
        "            self.window_size = min(self.num_patch)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        if self.shift_size > 0:\n",
        "            height, width = self.num_patch\n",
        "            h_slices = (slice(0, -self.window_size), slice(-self.window_size, -self.shift_size), slice(-self.shift_size, None))\n",
        "            w_slices = (slice(0, -self.window_size), slice(-self.window_size, -self.shift_size), slice(-self.shift_size, None))\n",
        "            mask_array = np.zeros((1, height, width, 1))\n",
        "            count = 0\n",
        "            for h in h_slices:\n",
        "                for w in w_slices:\n",
        "                    mask_array[:, h, w, :] = count\n",
        "                    count += 1\n",
        "            mask_array = ops.convert_to_tensor(mask_array)\n",
        "            mask_windows = window_partition(mask_array, self.window_size)\n",
        "            mask_windows = ops.reshape(mask_windows, [-1, self.window_size * self.window_size])\n",
        "            attn_mask = ops.expand_dims(mask_windows, axis=1) - ops.expand_dims(mask_windows, axis=2)\n",
        "            attn_mask = ops.where(attn_mask != 0, -100.0, attn_mask)\n",
        "            attn_mask = ops.where(attn_mask == 0, 0.0, attn_mask)\n",
        "            self.attn_mask = keras.Variable(initializer=attn_mask, shape=attn_mask.shape, dtype=attn_mask.dtype, trainable=False)\n",
        "        else:\n",
        "            self.attn_mask = None\n",
        "\n",
        "    def call(self, x, training=False):\n",
        "        height, width = self.num_patch\n",
        "        _, num_patches_before, channels = x.shape\n",
        "        x_skip = x\n",
        "        x = self.norm1(x)\n",
        "        x = ops.reshape(x, (-1, height, width, channels))\n",
        "        if self.shift_size > 0:\n",
        "            shifted_x = ops.roll(x, shift=[-self.shift_size, -self.shift_size], axis=[1, 2])\n",
        "        else:\n",
        "            shifted_x = x\n",
        "        x_windows = window_partition(shifted_x, self.window_size)\n",
        "        x_windows = ops.reshape(x_windows, (-1, self.window_size * self.window_size, channels))\n",
        "        attn_windows = self.attn(x_windows, mask=self.attn_mask)\n",
        "        attn_windows = ops.reshape(attn_windows, (-1, self.window_size, self.window_size, channels))\n",
        "        shifted_x = window_reverse(attn_windows, self.window_size, height, width, channels)\n",
        "        if self.shift_size > 0:\n",
        "            x = ops.roll(shifted_x, shift=[self.shift_size, self.shift_size], axis=[1, 2])\n",
        "        else:\n",
        "            x = shifted_x\n",
        "        x = ops.reshape(x, (-1, height * width, channels))\n",
        "        x = self.drop_path(x, training=training)\n",
        "        x = x_skip + x\n",
        "        x_skip = x\n",
        "        x = self.norm2(x)\n",
        "        x = self.mlp(x)\n",
        "        x = self.drop_path(x)\n",
        "        x = x_skip + x\n",
        "        return x\n",
        "\n",
        "class PatchEmbedding(layers.Layer):\n",
        "    def __init__(self, num_patch, embed_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.num_patch = num_patch\n",
        "        self.proj = layers.Dense(embed_dim)\n",
        "        self.pos_embed = layers.Embedding(input_dim=num_patch, output_dim=embed_dim)\n",
        "\n",
        "    def call(self, patch):\n",
        "        pos = ops.arange(start=0, stop=self.num_patch)\n",
        "        return self.proj(patch) + self.pos_embed(pos)\n",
        "\n",
        "class PatchMerging(keras.layers.Layer):\n",
        "    def __init__(self, num_patch, embed_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.num_patch = num_patch\n",
        "        self.embed_dim = embed_dim\n",
        "        self.linear_trans = layers.Dense(2 * embed_dim, use_bias=False)\n",
        "\n",
        "    def call(self, x):\n",
        "        height, width = self.num_patch\n",
        "        _, _, C = x.shape\n",
        "        x = ops.reshape(x, (-1, height, width, C))\n",
        "        x0 = x[:, 0::2, 0::2, :]\n",
        "        x1 = x[:, 1::2, 0::2, :]\n",
        "        x2 = x[:, 0::2, 1::2, :]\n",
        "        x3 = x[:, 1::2, 1::2, :]\n",
        "        x = ops.concatenate((x0, x1, x2, x3), axis=-1)\n",
        "        x = ops.reshape(x, (-1, (height // 2) * (width // 2), 4 * C))\n",
        "        return self.linear_trans(x)"
      ],
      "metadata": {
        "id": "y4ASxVmG39GD"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Assembly\n",
        "\n",
        "Now I will assemble the layers together to build the final model. For this first attempt, I'll use a simple structure with two SwinTransformer blocks followed by a patch merging layer and a final classification head."
      ],
      "metadata": {
        "id": "Edt6rS4e4HKY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Model Assembly ---\n",
        "patch_feature_dim = patch_size[0] * patch_size[1] * input_shape[2]\n",
        "num_patches_total = num_patch_x * num_patch_y\n",
        "input_layer_shape = (num_patches_total, patch_feature_dim)\n",
        "\n",
        "input_tensor = layers.Input(shape=input_layer_shape)\n",
        "\n",
        "# Stage 1\n",
        "x = PatchEmbedding(num_patches_total, embed_dim)(input_tensor)\n",
        "x = SwinTransformer(\n",
        "    dim=embed_dim,\n",
        "    num_patch=(num_patch_x, num_patch_y),\n",
        "    num_heads=num_heads,\n",
        "    window_size=window_size,\n",
        "    shift_size=0,\n",
        "    num_mlp=num_mlp,\n",
        "    name=\"swin_stage1_block1\"\n",
        ")(x)\n",
        "x = SwinTransformer(\n",
        "    dim=embed_dim,\n",
        "    num_patch=(num_patch_x, num_patch_y),\n",
        "    num_heads=num_heads,\n",
        "    window_size=window_size,\n",
        "    shift_size=shift_size,\n",
        "    num_mlp=num_mlp,\n",
        "    name=\"swin_stage1_block2\"\n",
        ")(x)\n",
        "\n",
        "# Downsampling\n",
        "x = PatchMerging((num_patch_x, num_patch_y), embed_dim=embed_dim)(x)\n",
        "\n",
        "# Final Classification Head\n",
        "x = layers.LayerNormalization(epsilon=1e-5)(x)\n",
        "x = layers.GlobalAveragePooling1D()(x)\n",
        "output = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
        "\n",
        "# Create the final model\n",
        "model = keras.Model(input_tensor, output)\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 475
        },
        "id": "GK78ksYh4HZG",
        "outputId": "74ab2b2f-77d9-452f-c37d-28430e4db805"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_2\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_2\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3136\u001b[0m, \u001b[38;5;34m48\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ patch_embedding                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3136\u001b[0m, \u001b[38;5;34m96\u001b[0m)       │       \u001b[38;5;34m305,760\u001b[0m │\n",
              "│ (\u001b[38;5;33mPatchEmbedding\u001b[0m)                │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ swin_stage1_block1              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3136\u001b[0m, \u001b[38;5;34m96\u001b[0m)       │       \u001b[38;5;34m115,255\u001b[0m │\n",
              "│ (\u001b[38;5;33mSwinTransformer\u001b[0m)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ swin_stage1_block2              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3136\u001b[0m, \u001b[38;5;34m96\u001b[0m)       │       \u001b[38;5;34m268,919\u001b[0m │\n",
              "│ (\u001b[38;5;33mSwinTransformer\u001b[0m)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ patch_merging (\u001b[38;5;33mPatchMerging\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m, \u001b[38;5;34m192\u001b[0m)       │        \u001b[38;5;34m73,728\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ layer_normalization_4           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m, \u001b[38;5;34m192\u001b[0m)       │           \u001b[38;5;34m384\u001b[0m │\n",
              "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling1d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_10 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)              │           \u001b[38;5;34m965\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3136</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ patch_embedding                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3136</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">305,760</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PatchEmbedding</span>)                │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ swin_stage1_block1              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3136</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">115,255</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SwinTransformer</span>)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ swin_stage1_block2              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3136</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">268,919</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SwinTransformer</span>)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ patch_merging (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PatchMerging</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,728</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ layer_normalization_4           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>)       │           <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling1d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">965</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m765,011\u001b[0m (3.52 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">765,011</span> (3.52 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m606,545\u001b[0m (2.31 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">606,545</span> (2.31 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m158,466\u001b[0m (1.21 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">158,466</span> (1.21 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 2: Data Loading and Initial Training Attempt\n",
        "\n",
        "With the model architecture defined, the next step is to load the pre-processed data and create the `tf.data` pipeline. This involves loading the NumPy arrays we saved earlier and setting up the data pipeline for training.\n",
        "\n",
        "After that, I will compile the model and run the first training experiment to get a baseline performance."
      ],
      "metadata": {
        "id": "tefsthPoBDwp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Training Hyperparameters ---\n",
        "learning_rate = 1e-3\n",
        "batch_size = 64\n",
        "num_epochs = 150\n",
        "weight_decay = 0.0001\n",
        "label_smoothing = 0.1\n",
        "\n",
        "# --- Load Pre-processed Data ---\n",
        "# Note: This assumes Google Drive is mounted.\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    DRIVE_LOAD_DIR = \"/content/drive/MyDrive/Colab_Datasets/APTOS_Processed_Augmented\"\n",
        "    x_train = np.load(os.path.join(DRIVE_LOAD_DIR, 'X_train.npy'))\n",
        "    y_train = np.load(os.path.join(DRIVE_LOAD_DIR, 'y_train_one_hot.npy'))\n",
        "    # For now, I will create a validation set from the training data for a quick check\n",
        "    print(f\"Loaded x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading data. Make sure you've run the preprocessing notebook. Details: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3gjR9Oz6BEdx",
        "outputId": "c1945389-0b11-4325-f1f4-a38f63286ff8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Loaded x_train shape: (8790, 224, 224, 3) - y_train shape: (8790, 5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create `tf.data` Pipeline\n",
        "\n",
        "I'm creating the data pipeline. One important step here is extracting patches from the images, because the Swin Transformer doesn't work on whole images directly. The `patch_extract_wrapper` function handles this."
      ],
      "metadata": {
        "id": "B1_gQH7hBLCY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This function extracts patches from images, a required step for the model\n",
        "def patch_extract_wrapper(images):\n",
        "    batch_size = tf.shape(images)[0]\n",
        "    patches = tf.image.extract_patches(\n",
        "        images=images,\n",
        "        sizes=(1, patch_size[0], patch_size[1], 1),\n",
        "        strides=(1, patch_size[0], patch_size[1], 1),\n",
        "        rates=(1, 1, 1, 1),\n",
        "        padding=\"VALID\",\n",
        "    )\n",
        "    patch_dim = patches.shape[-1]\n",
        "    patch_num = patches.shape[1]\n",
        "    return tf.reshape(patches, (batch_size, patch_num * patch_num, patch_dim))\n",
        "\n",
        "# Creating a validation split from the training data for this run\n",
        "val_split_index = int(len(x_train) * 0.9)\n",
        "x_val_split = x_train[val_split_index:]\n",
        "y_val_split = y_train[val_split_index:]\n",
        "x_train_split = x_train[:val_split_index]\n",
        "y_train_split = y_train[:val_split_index]\n",
        "\n",
        "\n",
        "# Create the training dataset pipeline\n",
        "dataset = (\n",
        "    tf.data.Dataset.from_tensor_slices((x_train_split, y_train_split))\n",
        "    .batch(batch_size=batch_size)\n",
        "    .map(lambda x, y: (patch_extract_wrapper(x), y), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    .prefetch(tf.data.AUTOTUNE)\n",
        ")\n",
        "\n",
        "# Create the validation dataset pipeline\n",
        "dataset_val = (\n",
        "    tf.data.Dataset.from_tensor_slices((x_val_split, y_val_split))\n",
        "    .batch(batch_size=batch_size)\n",
        "    .map(lambda x, y: (patch_extract_wrapper(x), y), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    .prefetch(tf.data.AUTOTUNE)\n",
        ")\n",
        "\n",
        "print(\"Training and validation tf.data pipelines created.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iuTZ4boQBL6F",
        "outputId": "8711da4f-4967-4358-fafa-d9e5cf1868bc"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training and validation tf.data pipelines created.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compile and Train the Model\n",
        "\n",
        "Now, everything is ready. I'll compile the model with the AdamW optimizer and categorical cross-entropy loss. Then, I will start the first training run."
      ],
      "metadata": {
        "id": "lZazOKOMBclx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Update: Resolving a Memory Error\n",
        "\n",
        "After running the initial setup, the training failed due to a `ResourceExhaustedError`. Even with Google Colab Pro, the `batch_size` of 128 proved to be too demanding for the GPU. To fix this, I'm reducing the batch size to a more reasonable value of 64. I will now restart the training."
      ],
      "metadata": {
        "id": "xcQFS4gFdWzv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "model.compile(\n",
        "    loss=keras.losses.CategoricalCrossentropy(label_smoothing=label_smoothing),\n",
        "    optimizer=keras.optimizers.AdamW(\n",
        "        learning_rate=learning_rate, weight_decay=weight_decay\n",
        "    ),\n",
        "    metrics=[\n",
        "        keras.metrics.CategoricalAccuracy(name=\"accuracy\"),\n",
        "        keras.metrics.TopKCategoricalAccuracy(2, name=\"top-2-accuracy\"),\n",
        "    ],\n",
        ")\n",
        "\n",
        "# Start training\n",
        "print(\"\\nStarting the first training experiment...\")\n",
        "history = model.fit(\n",
        "    dataset,\n",
        "    batch_size=batch_size,\n",
        "    epochs=num_epochs,\n",
        "    validation_data=dataset_val,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "688BymVPBdQY",
        "outputId": "4b9375de-578c-4a76-e00d-d9c893325244"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting the first training experiment...\n",
            "Epoch 1/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 394ms/step - accuracy: 0.4366 - loss: 1.6350 - top-2-accuracy: 0.6984 - val_accuracy: 0.6564 - val_loss: 1.1385 - val_top-2-accuracy: 0.8180\n",
            "Epoch 2/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 287ms/step - accuracy: 0.6291 - loss: 1.1702 - top-2-accuracy: 0.7836 - val_accuracy: 0.6655 - val_loss: 1.0978 - val_top-2-accuracy: 0.8191\n",
            "Epoch 3/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 277ms/step - accuracy: 0.6552 - loss: 1.1073 - top-2-accuracy: 0.8119 - val_accuracy: 0.6803 - val_loss: 1.0738 - val_top-2-accuracy: 0.8237\n",
            "Epoch 4/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.6696 - loss: 1.0780 - top-2-accuracy: 0.8221 - val_accuracy: 0.6860 - val_loss: 1.0707 - val_top-2-accuracy: 0.8225\n",
            "Epoch 5/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 281ms/step - accuracy: 0.6724 - loss: 1.0722 - top-2-accuracy: 0.8246 - val_accuracy: 0.6758 - val_loss: 1.0601 - val_top-2-accuracy: 0.8259\n",
            "Epoch 6/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 281ms/step - accuracy: 0.6764 - loss: 1.0630 - top-2-accuracy: 0.8218 - val_accuracy: 0.6860 - val_loss: 1.0538 - val_top-2-accuracy: 0.8259\n",
            "Epoch 7/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.6830 - loss: 1.0470 - top-2-accuracy: 0.8290 - val_accuracy: 0.6792 - val_loss: 1.0446 - val_top-2-accuracy: 0.8294\n",
            "Epoch 8/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 281ms/step - accuracy: 0.6915 - loss: 1.0287 - top-2-accuracy: 0.8374 - val_accuracy: 0.6940 - val_loss: 1.0384 - val_top-2-accuracy: 0.8328\n",
            "Epoch 9/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 281ms/step - accuracy: 0.6967 - loss: 1.0177 - top-2-accuracy: 0.8430 - val_accuracy: 0.6837 - val_loss: 1.0452 - val_top-2-accuracy: 0.8373\n",
            "Epoch 10/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.6982 - loss: 1.0087 - top-2-accuracy: 0.8467 - val_accuracy: 0.6792 - val_loss: 1.0556 - val_top-2-accuracy: 0.8294\n",
            "Epoch 11/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.7086 - loss: 0.9972 - top-2-accuracy: 0.8511 - val_accuracy: 0.6883 - val_loss: 1.0424 - val_top-2-accuracy: 0.8350\n",
            "Epoch 12/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 281ms/step - accuracy: 0.7114 - loss: 0.9938 - top-2-accuracy: 0.8470 - val_accuracy: 0.6894 - val_loss: 1.0425 - val_top-2-accuracy: 0.8294\n",
            "Epoch 13/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 281ms/step - accuracy: 0.7090 - loss: 0.9937 - top-2-accuracy: 0.8504 - val_accuracy: 0.6803 - val_loss: 1.0589 - val_top-2-accuracy: 0.8191\n",
            "Epoch 14/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 281ms/step - accuracy: 0.7119 - loss: 0.9895 - top-2-accuracy: 0.8523 - val_accuracy: 0.6849 - val_loss: 1.0454 - val_top-2-accuracy: 0.8328\n",
            "Epoch 15/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.7161 - loss: 0.9845 - top-2-accuracy: 0.8534 - val_accuracy: 0.6860 - val_loss: 1.0410 - val_top-2-accuracy: 0.8305\n",
            "Epoch 16/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.7163 - loss: 0.9761 - top-2-accuracy: 0.8584 - val_accuracy: 0.6837 - val_loss: 1.0447 - val_top-2-accuracy: 0.8328\n",
            "Epoch 17/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 281ms/step - accuracy: 0.7193 - loss: 0.9682 - top-2-accuracy: 0.8580 - val_accuracy: 0.6997 - val_loss: 1.0331 - val_top-2-accuracy: 0.8385\n",
            "Epoch 18/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 281ms/step - accuracy: 0.7235 - loss: 0.9595 - top-2-accuracy: 0.8591 - val_accuracy: 0.6746 - val_loss: 1.0409 - val_top-2-accuracy: 0.8294\n",
            "Epoch 19/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 281ms/step - accuracy: 0.7224 - loss: 0.9598 - top-2-accuracy: 0.8597 - val_accuracy: 0.6758 - val_loss: 1.0391 - val_top-2-accuracy: 0.8316\n",
            "Epoch 20/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 281ms/step - accuracy: 0.7305 - loss: 0.9500 - top-2-accuracy: 0.8636 - val_accuracy: 0.6860 - val_loss: 1.0480 - val_top-2-accuracy: 0.8385\n",
            "Epoch 21/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.7289 - loss: 0.9420 - top-2-accuracy: 0.8665 - val_accuracy: 0.6906 - val_loss: 1.0432 - val_top-2-accuracy: 0.8373\n",
            "Epoch 22/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.7346 - loss: 0.9372 - top-2-accuracy: 0.8619 - val_accuracy: 0.6849 - val_loss: 1.0470 - val_top-2-accuracy: 0.8328\n",
            "Epoch 23/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.7329 - loss: 0.9317 - top-2-accuracy: 0.8682 - val_accuracy: 0.6871 - val_loss: 1.0387 - val_top-2-accuracy: 0.8373\n",
            "Epoch 24/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.7359 - loss: 0.9308 - top-2-accuracy: 0.8707 - val_accuracy: 0.6780 - val_loss: 1.0634 - val_top-2-accuracy: 0.8362\n",
            "Epoch 25/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.7449 - loss: 0.9191 - top-2-accuracy: 0.8729 - val_accuracy: 0.6780 - val_loss: 1.0607 - val_top-2-accuracy: 0.8203\n",
            "Epoch 26/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 281ms/step - accuracy: 0.7527 - loss: 0.9109 - top-2-accuracy: 0.8742 - val_accuracy: 0.6712 - val_loss: 1.0577 - val_top-2-accuracy: 0.8248\n",
            "Epoch 27/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 281ms/step - accuracy: 0.7512 - loss: 0.9101 - top-2-accuracy: 0.8765 - val_accuracy: 0.6724 - val_loss: 1.0806 - val_top-2-accuracy: 0.8180\n",
            "Epoch 28/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 281ms/step - accuracy: 0.7519 - loss: 0.9093 - top-2-accuracy: 0.8769 - val_accuracy: 0.6906 - val_loss: 1.0686 - val_top-2-accuracy: 0.8339\n",
            "Epoch 29/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.7514 - loss: 0.9144 - top-2-accuracy: 0.8769 - val_accuracy: 0.6780 - val_loss: 1.0826 - val_top-2-accuracy: 0.8157\n",
            "Epoch 30/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.7510 - loss: 0.9076 - top-2-accuracy: 0.8814 - val_accuracy: 0.6780 - val_loss: 1.0741 - val_top-2-accuracy: 0.8180\n",
            "Epoch 31/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 281ms/step - accuracy: 0.7565 - loss: 0.8953 - top-2-accuracy: 0.8838 - val_accuracy: 0.6849 - val_loss: 1.0767 - val_top-2-accuracy: 0.8146\n",
            "Epoch 32/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 281ms/step - accuracy: 0.7633 - loss: 0.8947 - top-2-accuracy: 0.8863 - val_accuracy: 0.6917 - val_loss: 1.0690 - val_top-2-accuracy: 0.8282\n",
            "Epoch 33/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.7633 - loss: 0.8918 - top-2-accuracy: 0.8860 - val_accuracy: 0.6701 - val_loss: 1.0927 - val_top-2-accuracy: 0.8066\n",
            "Epoch 34/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.7669 - loss: 0.8853 - top-2-accuracy: 0.8861 - val_accuracy: 0.6792 - val_loss: 1.0749 - val_top-2-accuracy: 0.8077\n",
            "Epoch 35/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.7688 - loss: 0.8855 - top-2-accuracy: 0.8854 - val_accuracy: 0.6826 - val_loss: 1.0791 - val_top-2-accuracy: 0.8214\n",
            "Epoch 36/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.7701 - loss: 0.8800 - top-2-accuracy: 0.8876 - val_accuracy: 0.6917 - val_loss: 1.0806 - val_top-2-accuracy: 0.8191\n",
            "Epoch 37/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.7674 - loss: 0.8755 - top-2-accuracy: 0.8938 - val_accuracy: 0.6803 - val_loss: 1.0785 - val_top-2-accuracy: 0.8134\n",
            "Epoch 38/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 281ms/step - accuracy: 0.7703 - loss: 0.8706 - top-2-accuracy: 0.8989 - val_accuracy: 0.6894 - val_loss: 1.0832 - val_top-2-accuracy: 0.8180\n",
            "Epoch 39/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.7782 - loss: 0.8636 - top-2-accuracy: 0.8979 - val_accuracy: 0.6712 - val_loss: 1.1009 - val_top-2-accuracy: 0.8146\n",
            "Epoch 40/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.7815 - loss: 0.8583 - top-2-accuracy: 0.8972 - val_accuracy: 0.6758 - val_loss: 1.1044 - val_top-2-accuracy: 0.8077\n",
            "Epoch 41/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 281ms/step - accuracy: 0.7858 - loss: 0.8528 - top-2-accuracy: 0.8990 - val_accuracy: 0.6769 - val_loss: 1.1153 - val_top-2-accuracy: 0.8180\n",
            "Epoch 42/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.7842 - loss: 0.8548 - top-2-accuracy: 0.8964 - val_accuracy: 0.6871 - val_loss: 1.0992 - val_top-2-accuracy: 0.8282\n",
            "Epoch 43/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.7815 - loss: 0.8532 - top-2-accuracy: 0.8982 - val_accuracy: 0.6849 - val_loss: 1.0777 - val_top-2-accuracy: 0.8146\n",
            "Epoch 44/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.7780 - loss: 0.8568 - top-2-accuracy: 0.8984 - val_accuracy: 0.6735 - val_loss: 1.1049 - val_top-2-accuracy: 0.8055\n",
            "Epoch 45/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.7817 - loss: 0.8475 - top-2-accuracy: 0.9027 - val_accuracy: 0.6746 - val_loss: 1.0956 - val_top-2-accuracy: 0.8123\n",
            "Epoch 46/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 281ms/step - accuracy: 0.7884 - loss: 0.8408 - top-2-accuracy: 0.8999 - val_accuracy: 0.6815 - val_loss: 1.0857 - val_top-2-accuracy: 0.8066\n",
            "Epoch 47/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.7824 - loss: 0.8462 - top-2-accuracy: 0.9003 - val_accuracy: 0.6815 - val_loss: 1.0915 - val_top-2-accuracy: 0.8168\n",
            "Epoch 48/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.7912 - loss: 0.8267 - top-2-accuracy: 0.9077 - val_accuracy: 0.6769 - val_loss: 1.1309 - val_top-2-accuracy: 0.8043\n",
            "Epoch 49/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.7942 - loss: 0.8323 - top-2-accuracy: 0.8983 - val_accuracy: 0.6906 - val_loss: 1.0823 - val_top-2-accuracy: 0.8294\n",
            "Epoch 50/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 281ms/step - accuracy: 0.7936 - loss: 0.8289 - top-2-accuracy: 0.9072 - val_accuracy: 0.6871 - val_loss: 1.1010 - val_top-2-accuracy: 0.8225\n",
            "Epoch 51/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.7986 - loss: 0.8171 - top-2-accuracy: 0.9109 - val_accuracy: 0.6815 - val_loss: 1.0948 - val_top-2-accuracy: 0.8146\n",
            "Epoch 52/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 281ms/step - accuracy: 0.8076 - loss: 0.8065 - top-2-accuracy: 0.9151 - val_accuracy: 0.6803 - val_loss: 1.0970 - val_top-2-accuracy: 0.8214\n",
            "Epoch 53/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 281ms/step - accuracy: 0.8062 - loss: 0.8080 - top-2-accuracy: 0.9099 - val_accuracy: 0.6871 - val_loss: 1.0851 - val_top-2-accuracy: 0.8305\n",
            "Epoch 54/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.8011 - loss: 0.8173 - top-2-accuracy: 0.9095 - val_accuracy: 0.6689 - val_loss: 1.1181 - val_top-2-accuracy: 0.8111\n",
            "Epoch 55/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.8044 - loss: 0.8056 - top-2-accuracy: 0.9132 - val_accuracy: 0.6735 - val_loss: 1.1105 - val_top-2-accuracy: 0.8191\n",
            "Epoch 56/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.7985 - loss: 0.8096 - top-2-accuracy: 0.9128 - val_accuracy: 0.6644 - val_loss: 1.1207 - val_top-2-accuracy: 0.8032\n",
            "Epoch 57/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.8064 - loss: 0.8016 - top-2-accuracy: 0.9152 - val_accuracy: 0.6871 - val_loss: 1.1009 - val_top-2-accuracy: 0.8134\n",
            "Epoch 58/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.8078 - loss: 0.7974 - top-2-accuracy: 0.9213 - val_accuracy: 0.6735 - val_loss: 1.1386 - val_top-2-accuracy: 0.8032\n",
            "Epoch 59/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 281ms/step - accuracy: 0.8147 - loss: 0.7870 - top-2-accuracy: 0.9211 - val_accuracy: 0.6542 - val_loss: 1.1349 - val_top-2-accuracy: 0.8009\n",
            "Epoch 60/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.8135 - loss: 0.7903 - top-2-accuracy: 0.9215 - val_accuracy: 0.6746 - val_loss: 1.1267 - val_top-2-accuracy: 0.8123\n",
            "Epoch 61/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.8156 - loss: 0.7868 - top-2-accuracy: 0.9215 - val_accuracy: 0.6712 - val_loss: 1.1244 - val_top-2-accuracy: 0.8089\n",
            "Epoch 62/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.8301 - loss: 0.7708 - top-2-accuracy: 0.9275 - val_accuracy: 0.6860 - val_loss: 1.0968 - val_top-2-accuracy: 0.8203\n",
            "Epoch 63/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.8290 - loss: 0.7639 - top-2-accuracy: 0.9301 - val_accuracy: 0.6917 - val_loss: 1.0943 - val_top-2-accuracy: 0.8203\n",
            "Epoch 64/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 281ms/step - accuracy: 0.8324 - loss: 0.7587 - top-2-accuracy: 0.9313 - val_accuracy: 0.6917 - val_loss: 1.1053 - val_top-2-accuracy: 0.8305\n",
            "Epoch 65/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.8385 - loss: 0.7452 - top-2-accuracy: 0.9378 - val_accuracy: 0.6735 - val_loss: 1.1024 - val_top-2-accuracy: 0.8066\n",
            "Epoch 66/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.8370 - loss: 0.7509 - top-2-accuracy: 0.9351 - val_accuracy: 0.6655 - val_loss: 1.1121 - val_top-2-accuracy: 0.8066\n",
            "Epoch 67/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.8448 - loss: 0.7444 - top-2-accuracy: 0.9352 - val_accuracy: 0.6837 - val_loss: 1.1092 - val_top-2-accuracy: 0.8157\n",
            "Epoch 68/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.8448 - loss: 0.7367 - top-2-accuracy: 0.9393 - val_accuracy: 0.6803 - val_loss: 1.1205 - val_top-2-accuracy: 0.8123\n",
            "Epoch 69/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.8472 - loss: 0.7306 - top-2-accuracy: 0.9370 - val_accuracy: 0.6724 - val_loss: 1.1434 - val_top-2-accuracy: 0.8089\n",
            "Epoch 70/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.8486 - loss: 0.7323 - top-2-accuracy: 0.9422 - val_accuracy: 0.6826 - val_loss: 1.1515 - val_top-2-accuracy: 0.8032\n",
            "Epoch 71/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.8514 - loss: 0.7228 - top-2-accuracy: 0.9429 - val_accuracy: 0.6621 - val_loss: 1.1612 - val_top-2-accuracy: 0.8168\n",
            "Epoch 72/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.8548 - loss: 0.7249 - top-2-accuracy: 0.9448 - val_accuracy: 0.6667 - val_loss: 1.2068 - val_top-2-accuracy: 0.8214\n",
            "Epoch 73/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.8565 - loss: 0.7243 - top-2-accuracy: 0.9455 - val_accuracy: 0.6792 - val_loss: 1.1551 - val_top-2-accuracy: 0.8191\n",
            "Epoch 74/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 281ms/step - accuracy: 0.8646 - loss: 0.7058 - top-2-accuracy: 0.9495 - val_accuracy: 0.6598 - val_loss: 1.2064 - val_top-2-accuracy: 0.8146\n",
            "Epoch 75/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 281ms/step - accuracy: 0.8641 - loss: 0.7065 - top-2-accuracy: 0.9508 - val_accuracy: 0.6610 - val_loss: 1.1916 - val_top-2-accuracy: 0.7941\n",
            "Epoch 76/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.8596 - loss: 0.7139 - top-2-accuracy: 0.9463 - val_accuracy: 0.6644 - val_loss: 1.1940 - val_top-2-accuracy: 0.8111\n",
            "Epoch 77/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.8685 - loss: 0.6970 - top-2-accuracy: 0.9498 - val_accuracy: 0.6735 - val_loss: 1.2004 - val_top-2-accuracy: 0.8009\n",
            "Epoch 78/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.8766 - loss: 0.6900 - top-2-accuracy: 0.9514 - val_accuracy: 0.6576 - val_loss: 1.1876 - val_top-2-accuracy: 0.8009\n",
            "Epoch 79/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.8751 - loss: 0.6884 - top-2-accuracy: 0.9531 - val_accuracy: 0.6542 - val_loss: 1.2178 - val_top-2-accuracy: 0.8111\n",
            "Epoch 80/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 281ms/step - accuracy: 0.8735 - loss: 0.6876 - top-2-accuracy: 0.9528 - val_accuracy: 0.6519 - val_loss: 1.2151 - val_top-2-accuracy: 0.7975\n",
            "Epoch 81/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.8754 - loss: 0.6843 - top-2-accuracy: 0.9571 - val_accuracy: 0.6724 - val_loss: 1.1958 - val_top-2-accuracy: 0.7998\n",
            "Epoch 82/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.8774 - loss: 0.6827 - top-2-accuracy: 0.9511 - val_accuracy: 0.6621 - val_loss: 1.1895 - val_top-2-accuracy: 0.8294\n",
            "Epoch 83/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.8845 - loss: 0.6698 - top-2-accuracy: 0.9600 - val_accuracy: 0.6724 - val_loss: 1.1740 - val_top-2-accuracy: 0.8089\n",
            "Epoch 84/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.8907 - loss: 0.6614 - top-2-accuracy: 0.9626 - val_accuracy: 0.6621 - val_loss: 1.2103 - val_top-2-accuracy: 0.8077\n",
            "Epoch 85/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.8819 - loss: 0.6657 - top-2-accuracy: 0.9606 - val_accuracy: 0.6758 - val_loss: 1.1740 - val_top-2-accuracy: 0.8146\n",
            "Epoch 86/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.8868 - loss: 0.6637 - top-2-accuracy: 0.9624 - val_accuracy: 0.6428 - val_loss: 1.2382 - val_top-2-accuracy: 0.7850\n",
            "Epoch 87/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.8852 - loss: 0.6636 - top-2-accuracy: 0.9604 - val_accuracy: 0.6428 - val_loss: 1.2387 - val_top-2-accuracy: 0.7895\n",
            "Epoch 88/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.8926 - loss: 0.6573 - top-2-accuracy: 0.9612 - val_accuracy: 0.6473 - val_loss: 1.2335 - val_top-2-accuracy: 0.7975\n",
            "Epoch 89/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.8964 - loss: 0.6474 - top-2-accuracy: 0.9640 - val_accuracy: 0.6553 - val_loss: 1.2321 - val_top-2-accuracy: 0.8066\n",
            "Epoch 90/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.8970 - loss: 0.6448 - top-2-accuracy: 0.9650 - val_accuracy: 0.6416 - val_loss: 1.2561 - val_top-2-accuracy: 0.8077\n",
            "Epoch 91/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.8840 - loss: 0.6568 - top-2-accuracy: 0.9666 - val_accuracy: 0.6280 - val_loss: 1.2203 - val_top-2-accuracy: 0.7850\n",
            "Epoch 92/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.8875 - loss: 0.6593 - top-2-accuracy: 0.9642 - val_accuracy: 0.6553 - val_loss: 1.2005 - val_top-2-accuracy: 0.7952\n",
            "Epoch 93/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.9023 - loss: 0.6358 - top-2-accuracy: 0.9711 - val_accuracy: 0.6451 - val_loss: 1.2484 - val_top-2-accuracy: 0.7941\n",
            "Epoch 94/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.8992 - loss: 0.6334 - top-2-accuracy: 0.9665 - val_accuracy: 0.6564 - val_loss: 1.2180 - val_top-2-accuracy: 0.8032\n",
            "Epoch 95/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.9030 - loss: 0.6230 - top-2-accuracy: 0.9749 - val_accuracy: 0.6485 - val_loss: 1.2483 - val_top-2-accuracy: 0.8134\n",
            "Epoch 96/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.9117 - loss: 0.6257 - top-2-accuracy: 0.9724 - val_accuracy: 0.6667 - val_loss: 1.2101 - val_top-2-accuracy: 0.7986\n",
            "Epoch 97/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 283ms/step - accuracy: 0.9028 - loss: 0.6281 - top-2-accuracy: 0.9723 - val_accuracy: 0.6530 - val_loss: 1.2521 - val_top-2-accuracy: 0.7941\n",
            "Epoch 98/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.9123 - loss: 0.6190 - top-2-accuracy: 0.9758 - val_accuracy: 0.6519 - val_loss: 1.2386 - val_top-2-accuracy: 0.7918\n",
            "Epoch 99/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.9158 - loss: 0.6076 - top-2-accuracy: 0.9788 - val_accuracy: 0.6428 - val_loss: 1.2653 - val_top-2-accuracy: 0.7804\n",
            "Epoch 100/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.9152 - loss: 0.6104 - top-2-accuracy: 0.9759 - val_accuracy: 0.6485 - val_loss: 1.2804 - val_top-2-accuracy: 0.7759\n",
            "Epoch 101/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.9184 - loss: 0.6085 - top-2-accuracy: 0.9755 - val_accuracy: 0.6507 - val_loss: 1.2740 - val_top-2-accuracy: 0.7929\n",
            "Epoch 102/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.9193 - loss: 0.6038 - top-2-accuracy: 0.9793 - val_accuracy: 0.6610 - val_loss: 1.2482 - val_top-2-accuracy: 0.7986\n",
            "Epoch 103/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.9215 - loss: 0.5953 - top-2-accuracy: 0.9802 - val_accuracy: 0.6598 - val_loss: 1.2398 - val_top-2-accuracy: 0.7884\n",
            "Epoch 104/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.9230 - loss: 0.6002 - top-2-accuracy: 0.9801 - val_accuracy: 0.6496 - val_loss: 1.2548 - val_top-2-accuracy: 0.8043\n",
            "Epoch 105/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.9172 - loss: 0.6055 - top-2-accuracy: 0.9771 - val_accuracy: 0.6439 - val_loss: 1.2702 - val_top-2-accuracy: 0.7759\n",
            "Epoch 106/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.9274 - loss: 0.5940 - top-2-accuracy: 0.9792 - val_accuracy: 0.6462 - val_loss: 1.2635 - val_top-2-accuracy: 0.7998\n",
            "Epoch 107/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.9310 - loss: 0.5870 - top-2-accuracy: 0.9793 - val_accuracy: 0.6519 - val_loss: 1.2994 - val_top-2-accuracy: 0.8066\n",
            "Epoch 108/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.9335 - loss: 0.5789 - top-2-accuracy: 0.9841 - val_accuracy: 0.6553 - val_loss: 1.2553 - val_top-2-accuracy: 0.8055\n",
            "Epoch 109/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.9322 - loss: 0.5775 - top-2-accuracy: 0.9849 - val_accuracy: 0.6598 - val_loss: 1.2807 - val_top-2-accuracy: 0.8134\n",
            "Epoch 110/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 281ms/step - accuracy: 0.9275 - loss: 0.5860 - top-2-accuracy: 0.9832 - val_accuracy: 0.6473 - val_loss: 1.2985 - val_top-2-accuracy: 0.7952\n",
            "Epoch 111/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 281ms/step - accuracy: 0.9386 - loss: 0.5692 - top-2-accuracy: 0.9841 - val_accuracy: 0.6371 - val_loss: 1.2848 - val_top-2-accuracy: 0.7986\n",
            "Epoch 112/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.9422 - loss: 0.5702 - top-2-accuracy: 0.9852 - val_accuracy: 0.6530 - val_loss: 1.2630 - val_top-2-accuracy: 0.8009\n",
            "Epoch 113/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.9321 - loss: 0.5758 - top-2-accuracy: 0.9832 - val_accuracy: 0.6678 - val_loss: 1.2758 - val_top-2-accuracy: 0.8009\n",
            "Epoch 114/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.9354 - loss: 0.5758 - top-2-accuracy: 0.9841 - val_accuracy: 0.6655 - val_loss: 1.2937 - val_top-2-accuracy: 0.7998\n",
            "Epoch 115/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.9357 - loss: 0.5706 - top-2-accuracy: 0.9872 - val_accuracy: 0.6348 - val_loss: 1.3364 - val_top-2-accuracy: 0.8055\n",
            "Epoch 116/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.9390 - loss: 0.5685 - top-2-accuracy: 0.9862 - val_accuracy: 0.6246 - val_loss: 1.3037 - val_top-2-accuracy: 0.7850\n",
            "Epoch 117/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.9278 - loss: 0.5926 - top-2-accuracy: 0.9812 - val_accuracy: 0.6542 - val_loss: 1.2479 - val_top-2-accuracy: 0.7918\n",
            "Epoch 118/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.9326 - loss: 0.5840 - top-2-accuracy: 0.9852 - val_accuracy: 0.6655 - val_loss: 1.2505 - val_top-2-accuracy: 0.8123\n",
            "Epoch 119/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.9344 - loss: 0.5822 - top-2-accuracy: 0.9836 - val_accuracy: 0.6530 - val_loss: 1.2915 - val_top-2-accuracy: 0.8089\n",
            "Epoch 120/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.9347 - loss: 0.5731 - top-2-accuracy: 0.9855 - val_accuracy: 0.6621 - val_loss: 1.2856 - val_top-2-accuracy: 0.8020\n",
            "Epoch 121/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.9414 - loss: 0.5606 - top-2-accuracy: 0.9877 - val_accuracy: 0.6655 - val_loss: 1.2818 - val_top-2-accuracy: 0.8009\n",
            "Epoch 122/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.9449 - loss: 0.5561 - top-2-accuracy: 0.9878 - val_accuracy: 0.6746 - val_loss: 1.2614 - val_top-2-accuracy: 0.8157\n",
            "Epoch 123/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 281ms/step - accuracy: 0.9429 - loss: 0.5603 - top-2-accuracy: 0.9859 - val_accuracy: 0.6678 - val_loss: 1.2606 - val_top-2-accuracy: 0.8146\n",
            "Epoch 124/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.9447 - loss: 0.5594 - top-2-accuracy: 0.9874 - val_accuracy: 0.6701 - val_loss: 1.2636 - val_top-2-accuracy: 0.8123\n",
            "Epoch 125/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.9425 - loss: 0.5637 - top-2-accuracy: 0.9872 - val_accuracy: 0.6769 - val_loss: 1.2629 - val_top-2-accuracy: 0.8043\n",
            "Epoch 126/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.9423 - loss: 0.5585 - top-2-accuracy: 0.9867 - val_accuracy: 0.6439 - val_loss: 1.2966 - val_top-2-accuracy: 0.8020\n",
            "Epoch 127/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.9443 - loss: 0.5589 - top-2-accuracy: 0.9875 - val_accuracy: 0.6667 - val_loss: 1.3299 - val_top-2-accuracy: 0.8066\n",
            "Epoch 128/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.9455 - loss: 0.5587 - top-2-accuracy: 0.9866 - val_accuracy: 0.6542 - val_loss: 1.3239 - val_top-2-accuracy: 0.7793\n",
            "Epoch 129/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.9459 - loss: 0.5553 - top-2-accuracy: 0.9908 - val_accuracy: 0.6234 - val_loss: 1.3656 - val_top-2-accuracy: 0.7804\n",
            "Epoch 130/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.9518 - loss: 0.5461 - top-2-accuracy: 0.9887 - val_accuracy: 0.6268 - val_loss: 1.3039 - val_top-2-accuracy: 0.7850\n",
            "Epoch 131/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.9512 - loss: 0.5441 - top-2-accuracy: 0.9881 - val_accuracy: 0.6086 - val_loss: 1.4199 - val_top-2-accuracy: 0.7702\n",
            "Epoch 132/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 281ms/step - accuracy: 0.9537 - loss: 0.5449 - top-2-accuracy: 0.9892 - val_accuracy: 0.6189 - val_loss: 1.3349 - val_top-2-accuracy: 0.7861\n",
            "Epoch 133/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.9561 - loss: 0.5372 - top-2-accuracy: 0.9913 - val_accuracy: 0.6416 - val_loss: 1.3073 - val_top-2-accuracy: 0.7873\n",
            "Epoch 134/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.9501 - loss: 0.5468 - top-2-accuracy: 0.9898 - val_accuracy: 0.6530 - val_loss: 1.2826 - val_top-2-accuracy: 0.8032\n",
            "Epoch 135/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 281ms/step - accuracy: 0.9559 - loss: 0.5381 - top-2-accuracy: 0.9888 - val_accuracy: 0.6382 - val_loss: 1.2860 - val_top-2-accuracy: 0.7884\n",
            "Epoch 136/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.9545 - loss: 0.5361 - top-2-accuracy: 0.9923 - val_accuracy: 0.6223 - val_loss: 1.3087 - val_top-2-accuracy: 0.7895\n",
            "Epoch 137/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.9603 - loss: 0.5319 - top-2-accuracy: 0.9930 - val_accuracy: 0.6212 - val_loss: 1.3415 - val_top-2-accuracy: 0.7793\n",
            "Epoch 138/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.9638 - loss: 0.5277 - top-2-accuracy: 0.9928 - val_accuracy: 0.6303 - val_loss: 1.3263 - val_top-2-accuracy: 0.7782\n",
            "Epoch 139/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.9630 - loss: 0.5225 - top-2-accuracy: 0.9934 - val_accuracy: 0.6359 - val_loss: 1.2855 - val_top-2-accuracy: 0.8066\n",
            "Epoch 140/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.9641 - loss: 0.5223 - top-2-accuracy: 0.9919 - val_accuracy: 0.6473 - val_loss: 1.3013 - val_top-2-accuracy: 0.7975\n",
            "Epoch 141/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 281ms/step - accuracy: 0.9629 - loss: 0.5228 - top-2-accuracy: 0.9919 - val_accuracy: 0.6325 - val_loss: 1.3345 - val_top-2-accuracy: 0.7998\n",
            "Epoch 142/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.9640 - loss: 0.5197 - top-2-accuracy: 0.9935 - val_accuracy: 0.6428 - val_loss: 1.3351 - val_top-2-accuracy: 0.7918\n",
            "Epoch 143/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.9632 - loss: 0.5209 - top-2-accuracy: 0.9928 - val_accuracy: 0.6405 - val_loss: 1.3464 - val_top-2-accuracy: 0.7884\n",
            "Epoch 144/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.9627 - loss: 0.5243 - top-2-accuracy: 0.9925 - val_accuracy: 0.6462 - val_loss: 1.3324 - val_top-2-accuracy: 0.7918\n",
            "Epoch 145/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.9624 - loss: 0.5232 - top-2-accuracy: 0.9941 - val_accuracy: 0.6337 - val_loss: 1.3493 - val_top-2-accuracy: 0.7952\n",
            "Epoch 146/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.9636 - loss: 0.5200 - top-2-accuracy: 0.9932 - val_accuracy: 0.6348 - val_loss: 1.3480 - val_top-2-accuracy: 0.8066\n",
            "Epoch 147/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.9668 - loss: 0.5145 - top-2-accuracy: 0.9940 - val_accuracy: 0.6473 - val_loss: 1.3351 - val_top-2-accuracy: 0.7895\n",
            "Epoch 148/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.9696 - loss: 0.5098 - top-2-accuracy: 0.9945 - val_accuracy: 0.6507 - val_loss: 1.3326 - val_top-2-accuracy: 0.7861\n",
            "Epoch 149/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.9692 - loss: 0.5106 - top-2-accuracy: 0.9945 - val_accuracy: 0.6359 - val_loss: 1.2971 - val_top-2-accuracy: 0.7929\n",
            "Epoch 150/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 282ms/step - accuracy: 0.9727 - loss: 0.5032 - top-2-accuracy: 0.9950 - val_accuracy: 0.6359 - val_loss: 1.3203 - val_top-2-accuracy: 0.7918\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 3: Analyzing the First Training Results\n",
        "\n",
        "The training has now completed, and the results are not ideal. As can be seen from the logs above, the training accuracy climbed to over 97%, but the validation accuracy stalled around 68% and then started to fluctuate or even decrease. Similarly, the validation loss began to increase after an initial drop.\n",
        "\n",
        "This is a clear indication of **severe overfiting**. The model has memorized the training set but failed to generalize to new data. For the next attempt, I need to implement a mechanism to stop the training process before this overfitting becomes too extreme."
      ],
      "metadata": {
        "id": "T3DfhAapWdcu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 4: Visualizing Performance and Adding Evaluation Metrics\n",
        "\n",
        "To get a clearer picture of the overfitting problem, I'll now visualize the training history. I'm also adding more detailed evaluation functions to generate classification reports and confusion matrices for future experiments."
      ],
      "metadata": {
        "id": "z9uEkSwImscF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "def plot_training_history(history):\n",
        "    # Function to plot the training and validation curves\n",
        "    acc = history.history['accuracy']\n",
        "    val_acc = history.history['val_accuracy']\n",
        "    loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "    epochs = range(1, len(acc) + 1)\n",
        "\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs, acc, 'bo-', label='Training Accuracy')\n",
        "    plt.plot(epochs, val_acc, 'ro-', label='Validation Accuracy')\n",
        "    plt.title('Training and Validation Accuracy')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs, loss, 'bo-', label='Training Loss')\n",
        "    plt.plot(epochs, val_loss, 'ro-', label='Validation Loss')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "# Plot the curves from the completed training run\n",
        "print(\"Plotting the training history from the first experiment...\")\n",
        "#plot_training_history(history)\n",
        "\n",
        "\n",
        "\n",
        "def evaluate_model_on_dataset(model, dataset):\n",
        "    print(\"\\nGenerating predictions for evaluation...\")\n",
        "    y_pred_probs = model.predict(dataset)\n",
        "    y_pred_classes = np.argmax(y_pred_probs, axis=1)\n",
        "\n",
        "    y_true_classes = []\n",
        "    for _, labels in dataset:\n",
        "        y_true_classes.extend(np.argmax(labels.numpy(), axis=1))\n",
        "\n",
        "    y_true_classes = np.array(y_true_classes)\n",
        "\n",
        "    class_names = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferative']\n",
        "\n",
        "    # Classification Report\n",
        "    print(\"\\n--- Classification Report ---\")\n",
        "    print(classification_report(y_true_classes, y_pred_classes, target_names=class_names))\n",
        "\n",
        "    # Confusion Matrix\n",
        "    print(\"\\n--- Confusion Matrix ---\")\n",
        "    cm = confusion_matrix(y_true_classes, y_pred_classes)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.ylabel('Actual Label')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.show()\n",
        "\n",
        "print(\"Evaluation functions defined.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t365FYT9Wcmo",
        "outputId": "a614d32c-8e64-45b5-e3cc-da00407ed89e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Plotting the training history from the first experiment...\n",
            "Evaluation functions defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 5: Second Training Attempt with Callbacks\n",
        "\n",
        "Based on the analysis of the first run, the key issue was training for too long. To combat this, I will now introduce Keras Callbacks into the training process.\n",
        "\n",
        "1.  **`EarlyStopping`**: This will monitor the `val_loss` and stop the training automatically when it stops improving, preventing the model from overfitting.\n",
        "2.  **`ModelCheckpoint`**: This will save only the best version of the model (the one with the lowest `val_loss`) during the entire training process.\n",
        "\n",
        "This is a much more robust approach to training."
      ],
      "metadata": {
        "id": "keJZNRmbn1G0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "# Define the callbacks\n",
        "early_stopping_callback = EarlyStopping(\n",
        "    monitor='val_loss',       # Monitor the validation loss\n",
        "    patience=10,              # Stop training if val_loss doesn't improve for 10 consecutive epochs\n",
        "    verbose=1,                # Print messages when stopping\n",
        "    restore_best_weights=True # Restore model weights from the epoch with the best val_loss\n",
        ")\n",
        "\n",
        "# Define the path to save the best model\n",
        "best_model_path = \"/content/drive/MyDrive/Colab_Datasets/Swin_model_best_weights.keras\"\n",
        "\n",
        "model_checkpoint_callback = ModelCheckpoint(\n",
        "    filepath=best_model_path,\n",
        "    save_weights_only=False,\n",
        "    monitor='val_loss',\n",
        "    mode='min',\n",
        "    save_best_only=True, # Only save the model if `val_loss` has improved\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"Callbacks for Early Stopping and Model Checkpointing have been defined.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K7pItdOTnT6W",
        "outputId": "e7ed080b-2064-4dba-bb90-1e2ad091faea"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Callbacks for Early Stopping and Model Checkpointing have been defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Re-compiling and Re-training\n",
        "\n",
        "I will re-compile the model to reset its weights and then start the training process again, this time using the callbacks. I'm still setting `epochs` to a high number, but I expect the `EarlyStopping` callback to finish the training much earlier."
      ],
      "metadata": {
        "id": "fZ4ek1jDoETD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-compile the model to reset the optimizer and weights\n",
        "model.compile(\n",
        "    loss=keras.losses.CategoricalCrossentropy(label_smoothing=label_smoothing),\n",
        "    optimizer=keras.optimizers.AdamW(\n",
        "        learning_rate=learning_rate, weight_decay=weight_decay\n",
        "    ),\n",
        "    metrics=[\n",
        "        keras.metrics.CategoricalAccuracy(name=\"accuracy\"),\n",
        "        keras.metrics.TopKCategoricalAccuracy(2, name=\"top-2-accuracy\"),\n",
        "    ],\n",
        ")\n",
        "\n",
        "# Start the second, improved training run\n",
        "print(\"\\nStarting the second training experiment with callbacks...\")\n",
        "history_improved = model.fit(\n",
        "    dataset,\n",
        "    epochs=150, # Set a high number, but expect it to stop early\n",
        "    validation_data=dataset_val,\n",
        "    callbacks=[early_stopping_callback, model_checkpoint_callback] # Pass the callbacks here\n",
        ")\n",
        "\n",
        "print(\"\\n--- Improved Training Finished ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KMTH9Xf1oE3Q",
        "outputId": "182d8089-1f3f-428c-b553-a604fd2d6a21"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting the second training experiment with callbacks...\n",
            "Epoch 1/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 335ms/step - accuracy: 0.4405 - loss: 1.6949 - top-2-accuracy: 0.6923\n",
            "Epoch 1: val_loss improved from inf to 1.14579, saving model to /content/drive/MyDrive/Colab_Datasets/Swin_model_best_weights.keras\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 386ms/step - accuracy: 0.4410 - loss: 1.6926 - top-2-accuracy: 0.6926 - val_accuracy: 0.6371 - val_loss: 1.1458 - val_top-2-accuracy: 0.8157\n",
            "Epoch 2/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 285ms/step - accuracy: 0.6224 - loss: 1.1785 - top-2-accuracy: 0.7840\n",
            "Epoch 2: val_loss improved from 1.14579 to 1.09587, saving model to /content/drive/MyDrive/Colab_Datasets/Swin_model_best_weights.keras\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 298ms/step - accuracy: 0.6224 - loss: 1.1783 - top-2-accuracy: 0.7840 - val_accuracy: 0.6633 - val_loss: 1.0959 - val_top-2-accuracy: 0.8168\n",
            "Epoch 3/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 278ms/step - accuracy: 0.6420 - loss: 1.1221 - top-2-accuracy: 0.8076\n",
            "Epoch 3: val_loss improved from 1.09587 to 1.07084, saving model to /content/drive/MyDrive/Colab_Datasets/Swin_model_best_weights.keras\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 290ms/step - accuracy: 0.6420 - loss: 1.1220 - top-2-accuracy: 0.8077 - val_accuracy: 0.6689 - val_loss: 1.0708 - val_top-2-accuracy: 0.8271\n",
            "Epoch 4/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 281ms/step - accuracy: 0.6581 - loss: 1.0931 - top-2-accuracy: 0.8153\n",
            "Epoch 4: val_loss improved from 1.07084 to 1.06177, saving model to /content/drive/MyDrive/Colab_Datasets/Swin_model_best_weights.keras\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 294ms/step - accuracy: 0.6581 - loss: 1.0929 - top-2-accuracy: 0.8154 - val_accuracy: 0.6758 - val_loss: 1.0618 - val_top-2-accuracy: 0.8294\n",
            "Epoch 5/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 283ms/step - accuracy: 0.6735 - loss: 1.0692 - top-2-accuracy: 0.8277\n",
            "Epoch 5: val_loss improved from 1.06177 to 1.05600, saving model to /content/drive/MyDrive/Colab_Datasets/Swin_model_best_weights.keras\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 295ms/step - accuracy: 0.6735 - loss: 1.0691 - top-2-accuracy: 0.8277 - val_accuracy: 0.6712 - val_loss: 1.0560 - val_top-2-accuracy: 0.8294\n",
            "Epoch 6/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 281ms/step - accuracy: 0.6827 - loss: 1.0503 - top-2-accuracy: 0.8327\n",
            "Epoch 6: val_loss did not improve from 1.05600\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 292ms/step - accuracy: 0.6827 - loss: 1.0502 - top-2-accuracy: 0.8327 - val_accuracy: 0.6712 - val_loss: 1.0699 - val_top-2-accuracy: 0.8294\n",
            "Epoch 7/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 282ms/step - accuracy: 0.6875 - loss: 1.0404 - top-2-accuracy: 0.8325\n",
            "Epoch 7: val_loss did not improve from 1.05600\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 294ms/step - accuracy: 0.6875 - loss: 1.0403 - top-2-accuracy: 0.8325 - val_accuracy: 0.6746 - val_loss: 1.0716 - val_top-2-accuracy: 0.8339\n",
            "Epoch 8/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 283ms/step - accuracy: 0.6890 - loss: 1.0297 - top-2-accuracy: 0.8383\n",
            "Epoch 8: val_loss did not improve from 1.05600\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 294ms/step - accuracy: 0.6890 - loss: 1.0297 - top-2-accuracy: 0.8383 - val_accuracy: 0.6735 - val_loss: 1.0736 - val_top-2-accuracy: 0.8203\n",
            "Epoch 9/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 283ms/step - accuracy: 0.6934 - loss: 1.0337 - top-2-accuracy: 0.8421\n",
            "Epoch 9: val_loss did not improve from 1.05600\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 294ms/step - accuracy: 0.6934 - loss: 1.0336 - top-2-accuracy: 0.8420 - val_accuracy: 0.6780 - val_loss: 1.0649 - val_top-2-accuracy: 0.8339\n",
            "Epoch 10/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 282ms/step - accuracy: 0.6901 - loss: 1.0300 - top-2-accuracy: 0.8400\n",
            "Epoch 10: val_loss improved from 1.05600 to 1.05428, saving model to /content/drive/MyDrive/Colab_Datasets/Swin_model_best_weights.keras\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 295ms/step - accuracy: 0.6901 - loss: 1.0299 - top-2-accuracy: 0.8400 - val_accuracy: 0.6746 - val_loss: 1.0543 - val_top-2-accuracy: 0.8407\n",
            "Epoch 11/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 282ms/step - accuracy: 0.7017 - loss: 1.0088 - top-2-accuracy: 0.8455\n",
            "Epoch 11: val_loss did not improve from 1.05428\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 294ms/step - accuracy: 0.7017 - loss: 1.0087 - top-2-accuracy: 0.8455 - val_accuracy: 0.6792 - val_loss: 1.0680 - val_top-2-accuracy: 0.8350\n",
            "Epoch 12/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 282ms/step - accuracy: 0.7066 - loss: 0.9947 - top-2-accuracy: 0.8512\n",
            "Epoch 12: val_loss did not improve from 1.05428\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 294ms/step - accuracy: 0.7066 - loss: 0.9946 - top-2-accuracy: 0.8511 - val_accuracy: 0.6712 - val_loss: 1.0804 - val_top-2-accuracy: 0.8191\n",
            "Epoch 13/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 282ms/step - accuracy: 0.7125 - loss: 0.9841 - top-2-accuracy: 0.8518\n",
            "Epoch 13: val_loss did not improve from 1.05428\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 294ms/step - accuracy: 0.7125 - loss: 0.9840 - top-2-accuracy: 0.8517 - val_accuracy: 0.6826 - val_loss: 1.0882 - val_top-2-accuracy: 0.8225\n",
            "Epoch 14/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 283ms/step - accuracy: 0.7147 - loss: 0.9763 - top-2-accuracy: 0.8580\n",
            "Epoch 14: val_loss did not improve from 1.05428\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 294ms/step - accuracy: 0.7147 - loss: 0.9762 - top-2-accuracy: 0.8580 - val_accuracy: 0.6815 - val_loss: 1.0859 - val_top-2-accuracy: 0.8191\n",
            "Epoch 15/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 283ms/step - accuracy: 0.7194 - loss: 0.9647 - top-2-accuracy: 0.8622\n",
            "Epoch 15: val_loss did not improve from 1.05428\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 294ms/step - accuracy: 0.7194 - loss: 0.9647 - top-2-accuracy: 0.8622 - val_accuracy: 0.6871 - val_loss: 1.0705 - val_top-2-accuracy: 0.8157\n",
            "Epoch 16/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 283ms/step - accuracy: 0.7203 - loss: 0.9558 - top-2-accuracy: 0.8659\n",
            "Epoch 16: val_loss did not improve from 1.05428\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 294ms/step - accuracy: 0.7203 - loss: 0.9558 - top-2-accuracy: 0.8659 - val_accuracy: 0.6803 - val_loss: 1.0730 - val_top-2-accuracy: 0.8259\n",
            "Epoch 17/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 283ms/step - accuracy: 0.7233 - loss: 0.9552 - top-2-accuracy: 0.8622\n",
            "Epoch 17: val_loss did not improve from 1.05428\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 294ms/step - accuracy: 0.7233 - loss: 0.9551 - top-2-accuracy: 0.8622 - val_accuracy: 0.6780 - val_loss: 1.0889 - val_top-2-accuracy: 0.8123\n",
            "Epoch 18/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 283ms/step - accuracy: 0.7240 - loss: 0.9539 - top-2-accuracy: 0.8658\n",
            "Epoch 18: val_loss did not improve from 1.05428\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 294ms/step - accuracy: 0.7240 - loss: 0.9538 - top-2-accuracy: 0.8658 - val_accuracy: 0.6815 - val_loss: 1.1006 - val_top-2-accuracy: 0.8180\n",
            "Epoch 19/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 283ms/step - accuracy: 0.7286 - loss: 0.9471 - top-2-accuracy: 0.8668\n",
            "Epoch 19: val_loss did not improve from 1.05428\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 294ms/step - accuracy: 0.7287 - loss: 0.9470 - top-2-accuracy: 0.8668 - val_accuracy: 0.6655 - val_loss: 1.1089 - val_top-2-accuracy: 0.8157\n",
            "Epoch 20/150\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 282ms/step - accuracy: 0.7350 - loss: 0.9339 - top-2-accuracy: 0.8723\n",
            "Epoch 20: val_loss did not improve from 1.05428\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 294ms/step - accuracy: 0.7350 - loss: 0.9338 - top-2-accuracy: 0.8723 - val_accuracy: 0.6542 - val_loss: 1.1147 - val_top-2-accuracy: 0.8111\n",
            "Epoch 20: early stopping\n",
            "Restoring model weights from the end of the best epoch: 10.\n",
            "\n",
            "--- Improved Training Finished ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analyzing the Improved Training Run\n",
        "\n",
        "Now let's plot the history from this second run. I expect to see that the training stopped before the validation loss started to increase significantly."
      ],
      "metadata": {
        "id": "b3pVRQIgoJkE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the results of the second training run\n",
        "print(\"Plotting the training history from the second, improved experiment...\")\n",
        "plot_training_history(history_improved)\n",
        "\n",
        "# Load the best model that was saved by ModelCheckpoint\n",
        "print(f\"\\nLoading the best model saved at: {best_model_path}\")\n",
        "best_model = keras.models.load_model(best_model_path)\n",
        "\n",
        "# Finally, evaluate the *best* model on the validation set\n",
        "print(\"\\n--- Final Evaluation on Validation Set (using best model) ---\")\n",
        "evaluate_model(best_model, dataset_val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "E2OOf67uoHVw",
        "outputId": "c5004f3f-d871-47b6-d6df-d8482f1ec2a8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Plotting the training history from the second, improved experiment...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loading the best model saved at: /content/drive/MyDrive/Colab_Datasets/Swin_model_best_weights.keras\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "<class 'keras.src.models.functional.Functional'> could not be deserialized properly. Please ensure that components that are Python object instances (layers, models, etc.) returned by `get_config()` are explicitly deserialized in the model's `from_config()` method.\n\nconfig={'module': 'keras.src.models.functional', 'class_name': 'Functional', 'config': {}, 'registered_name': 'Functional', 'build_config': {'input_shape': None}, 'compile_config': {'optimizer': {'module': 'keras.optimizers', 'class_name': 'AdamW', 'config': {'name': 'adamw', 'learning_rate': 0.0010000000474974513, 'weight_decay': 0.0001, 'clipnorm': None, 'global_clipnorm': None, 'clipvalue': None, 'use_ema': False, 'ema_momentum': 0.99, 'ema_overwrite_frequency': None, 'loss_scale_factor': None, 'gradient_accumulation_steps': None, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}, 'registered_name': None}, 'loss': {'module': 'keras.losses', 'class_name': 'CategoricalCrossentropy', 'config': {'name': 'categorical_crossentropy', 'reduction': 'sum_over_batch_size', 'from_logits': False, 'label_smoothing': 0.1, 'axis': -1}, 'registered_name': None}, 'loss_weights': None, 'metrics': [{'module': 'keras.metrics', 'class_name': 'CategoricalAccuracy', 'config': {'name': 'accuracy', 'dtype': 'float32'}, 'registered_name': None}, {'module': 'keras.metrics', 'class_name': 'TopKCategoricalAccuracy', 'config': {'name': 'top-2-accuracy', 'dtype': 'float32', 'k': 2}, 'registered_name': None}], 'weighted_metrics': None, 'run_eagerly': False, 'steps_per_execution': 1, 'jit_compile': True}}.\n\nException encountered: Could not locate class 'PatchEmbedding'. Make sure custom classes are decorated with `@keras.saving.register_keras_serializable()`. Full object config: {'module': None, 'class_name': 'PatchEmbedding', 'config': {'name': 'patch_embedding', 'num_patch': 3136, 'embed_dim': 96, 'trainable': True, 'dtype': {'module': 'keras', 'class_name': 'DTypePolicy', 'config': {'name': 'float32'}, 'registered_name': None}}, 'registered_name': 'PatchEmbedding', 'build_config': {'input_shape': [None, 3136, 48]}, 'name': 'patch_embedding', 'inbound_nodes': [{'args': [{'class_name': '__keras_tensor__', 'config': {'shape': [None, 3136, 48], 'dtype': 'float32', 'keras_history': ['input_layer', 0, 0]}}], 'kwargs': {}}]}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/saving/serialization_lib.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(config, custom_objects, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    717\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 718\u001b[0;31m             \u001b[0minstance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minner_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    719\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/models/model.py\u001b[0m in \u001b[0;36mfrom_config\u001b[0;34m(cls, config, custom_objects)\u001b[0m\n\u001b[1;32m    650\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 651\u001b[0;31m             return functional_from_config(\n\u001b[0m\u001b[1;32m    652\u001b[0m                 \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/models/functional.py\u001b[0m in \u001b[0;36mfunctional_from_config\u001b[0;34m(cls, config, custom_objects)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer_data\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfunctional_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"layers\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m         \u001b[0mprocess_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/models/functional.py\u001b[0m in \u001b[0;36mprocess_layer\u001b[0;34m(layer_data)\u001b[0m\n\u001b[1;32m    526\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 527\u001b[0;31m             layer = serialization_lib.deserialize_keras_object(\n\u001b[0m\u001b[1;32m    528\u001b[0m                 \u001b[0mlayer_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/saving/serialization_lib.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(config, custom_objects, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    693\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 694\u001b[0;31m     cls = _retrieve_class_or_fn(\n\u001b[0m\u001b[1;32m    695\u001b[0m         \u001b[0mclass_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/saving/serialization_lib.py\u001b[0m in \u001b[0;36m_retrieve_class_or_fn\u001b[0;34m(name, registered_name, module, obj_type, full_config, custom_objects)\u001b[0m\n\u001b[1;32m    809\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 810\u001b[0;31m     raise TypeError(\n\u001b[0m\u001b[1;32m    811\u001b[0m         \u001b[0;34mf\"Could not locate {obj_type} '{name}'. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Could not locate class 'PatchEmbedding'. Make sure custom classes are decorated with `@keras.saving.register_keras_serializable()`. Full object config: {'module': None, 'class_name': 'PatchEmbedding', 'config': {'name': 'patch_embedding', 'num_patch': 3136, 'embed_dim': 96, 'trainable': True, 'dtype': {'module': 'keras', 'class_name': 'DTypePolicy', 'config': {'name': 'float32'}, 'registered_name': None}}, 'registered_name': 'PatchEmbedding', 'build_config': {'input_shape': [None, 3136, 48]}, 'name': 'patch_embedding', 'inbound_nodes': [{'args': [{'class_name': '__keras_tensor__', 'config': {'shape': [None, 3136, 48], 'dtype': 'float32', 'keras_history': ['input_layer', 0, 0]}}], 'kwargs': {}}]}",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-732794127.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Load the best model that was saved by ModelCheckpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nLoading the best model saved at: {best_model_path}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mbest_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_model_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Finally, evaluate the *best* model on the validation set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/saving/saving_api.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_keras_zip\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mis_keras_dir\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mis_hf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m         return saving_lib.load_model(\n\u001b[0m\u001b[1;32m    190\u001b[0m             \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m             \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/saving/saving_lib.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    368\u001b[0m             )\n\u001b[1;32m    369\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 370\u001b[0;31m             return _load_model_from_fileobj(\n\u001b[0m\u001b[1;32m    371\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msafe_mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/saving/saving_lib.py\u001b[0m in \u001b[0;36m_load_model_from_fileobj\u001b[0;34m(fileobj, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    445\u001b[0m             \u001b[0mconfig_json\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 447\u001b[0;31m         model = _model_from_config(\n\u001b[0m\u001b[1;32m    448\u001b[0m             \u001b[0mconfig_json\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msafe_mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/saving/saving_lib.py\u001b[0m in \u001b[0;36m_model_from_config\u001b[0;34m(config_json, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    434\u001b[0m     \u001b[0;31m# Construct the model from the configuration file in the archive.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mObjectSharingScope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 436\u001b[0;31m         model = deserialize_keras_object(\n\u001b[0m\u001b[1;32m    437\u001b[0m             \u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msafe_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msafe_mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/saving/serialization_lib.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(config, custom_objects, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    718\u001b[0m             \u001b[0minstance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minner_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 720\u001b[0;31m             raise TypeError(\n\u001b[0m\u001b[1;32m    721\u001b[0m                 \u001b[0;34mf\"{cls} could not be deserialized properly. Please\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m                 \u001b[0;34m\" ensure that components that are Python object\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: <class 'keras.src.models.functional.Functional'> could not be deserialized properly. Please ensure that components that are Python object instances (layers, models, etc.) returned by `get_config()` are explicitly deserialized in the model's `from_config()` method.\n\nconfig={'module': 'keras.src.models.functional', 'class_name': 'Functional', 'config': {}, 'registered_name': 'Functional', 'build_config': {'input_shape': None}, 'compile_config': {'optimizer': {'module': 'keras.optimizers', 'class_name': 'AdamW', 'config': {'name': 'adamw', 'learning_rate': 0.0010000000474974513, 'weight_decay': 0.0001, 'clipnorm': None, 'global_clipnorm': None, 'clipvalue': None, 'use_ema': False, 'ema_momentum': 0.99, 'ema_overwrite_frequency': None, 'loss_scale_factor': None, 'gradient_accumulation_steps': None, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}, 'registered_name': None}, 'loss': {'module': 'keras.losses', 'class_name': 'CategoricalCrossentropy', 'config': {'name': 'categorical_crossentropy', 'reduction': 'sum_over_batch_size', 'from_logits': False, 'label_smoothing': 0.1, 'axis': -1}, 'registered_name': None}, 'loss_weights': None, 'metrics': [{'module': 'keras.metrics', 'class_name': 'CategoricalAccuracy', 'config': {'name': 'accuracy', 'dtype': 'float32'}, 'registered_name': None}, {'module': 'keras.metrics', 'class_name': 'TopKCategoricalAccuracy', 'config': {'name': 'top-2-accuracy', 'dtype': 'float32', 'k': 2}, 'registered_name': None}], 'weighted_metrics': None, 'run_eagerly': False, 'steps_per_execution': 1, 'jit_compile': True}}.\n\nException encountered: Could not locate class 'PatchEmbedding'. Make sure custom classes are decorated with `@keras.saving.register_keras_serializable()`. Full object config: {'module': None, 'class_name': 'PatchEmbedding', 'config': {'name': 'patch_embedding', 'num_patch': 3136, 'embed_dim': 96, 'trainable': True, 'dtype': {'module': 'keras', 'class_name': 'DTypePolicy', 'config': {'name': 'float32'}, 'registered_name': None}}, 'registered_name': 'PatchEmbedding', 'build_config': {'input_shape': [None, 3136, 48]}, 'name': 'patch_embedding', 'inbound_nodes': [{'args': [{'class_name': '__keras_tensor__', 'config': {'shape': [None, 3136, 48], 'dtype': 'float32', 'keras_history': ['input_layer', 0, 0]}}], 'kwargs': {}}]}"
          ]
        }
      ]
    }
  ]
}